{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11882025,
          "sourceType": "datasetVersion",
          "datasetId": 7398577
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EIwxoNLEjkM2",
        "1YCBJyW3jkM4",
        "hpv-4JKNjkM4",
        "ZjCevs0MFaOm",
        "j8gmLiqVFd5T",
        "XGFaZXwEjkM6"
      ],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebecf1fe5c5e4ae896043620561f2c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2d5e682805948deb31a0531d9eb7aba",
              "IPY_MODEL_42442305382d4286971622e4bace4ac0",
              "IPY_MODEL_54cafaa7813e43a7bdbbc301d8374212"
            ],
            "layout": "IPY_MODEL_35c2664be18c48aa8dc44fafe84d2af2"
          }
        },
        "e2d5e682805948deb31a0531d9eb7aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7abaa74ac194ccf9199c82c90175a78",
            "placeholder": "​",
            "style": "IPY_MODEL_25a33609a22744bdae3a0068c3f15900",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "42442305382d4286971622e4bace4ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93f78eb4182c485dbca511bf47aaf516",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_181c5543d3be41318833a2772753ef83",
            "value": 2
          }
        },
        "54cafaa7813e43a7bdbbc301d8374212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2a559cf5e6c4f1aaac3186ed93809a5",
            "placeholder": "​",
            "style": "IPY_MODEL_b9e2e93f311b439da958b57cdcd3c1ef",
            "value": " 2/2 [00:16&lt;00:00,  7.44s/it]"
          }
        },
        "35c2664be18c48aa8dc44fafe84d2af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7abaa74ac194ccf9199c82c90175a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a33609a22744bdae3a0068c3f15900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93f78eb4182c485dbca511bf47aaf516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181c5543d3be41318833a2772753ef83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2a559cf5e6c4f1aaac3186ed93809a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e2e93f311b439da958b57cdcd3c1ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "010209eeabf3408280dc1c4ff8a65bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b373cd61c4f245fc861c2824d9b12be3",
              "IPY_MODEL_fec45749adaf444e9389e5fc046cc19e",
              "IPY_MODEL_c8a2b2aee7164d0eacda248b5c672dd8"
            ],
            "layout": "IPY_MODEL_28ec92afff844b3f9b9dc8a1c926fff5"
          }
        },
        "b373cd61c4f245fc861c2824d9b12be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddaaf535e4b140a98fdede358c87f8d0",
            "placeholder": "​",
            "style": "IPY_MODEL_a2c76874c54b44c49aff6c2a03fd6446",
            "value": "Map: 100%"
          }
        },
        "fec45749adaf444e9389e5fc046cc19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26aaeea44d7c4b57b252e11d4bdb198a",
            "max": 9600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5542ba2898764e4488875bc49ca071c7",
            "value": 9600
          }
        },
        "c8a2b2aee7164d0eacda248b5c672dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b8b056919a94d6e8a375bac426b4167",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ccfba572a74d2f874ccabda384fd66",
            "value": " 9600/9600 [00:00&lt;00:00, 26084.24 examples/s]"
          }
        },
        "28ec92afff844b3f9b9dc8a1c926fff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddaaf535e4b140a98fdede358c87f8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c76874c54b44c49aff6c2a03fd6446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26aaeea44d7c4b57b252e11d4bdb198a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5542ba2898764e4488875bc49ca071c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b8b056919a94d6e8a375bac426b4167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ccfba572a74d2f874ccabda384fd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06fe9787c4b2487a92ca44eff04686ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3f1e2ec3b3e460d86776811d4eea493",
              "IPY_MODEL_52a8cecaaf964698acf2b85574c6d36c",
              "IPY_MODEL_85c97de3ae644fae8ba2bc32e49cba2d"
            ],
            "layout": "IPY_MODEL_af09f294119840a480987887a400cb1f"
          }
        },
        "e3f1e2ec3b3e460d86776811d4eea493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6a11d6ec3d4609aa99c54960d8096a",
            "placeholder": "​",
            "style": "IPY_MODEL_05dc0e0ec8f44f72b22c36492a9d9ce9",
            "value": "Epoch 0: 100%"
          }
        },
        "52a8cecaaf964698acf2b85574c6d36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a9ffa311e048069270d4e424ba75c8",
            "max": 9598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1abc51e8c0bb4bc1bf893bddce805b60",
            "value": 9598
          }
        },
        "85c97de3ae644fae8ba2bc32e49cba2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61fc685c4c4a437a863837e46cfe9683",
            "placeholder": "​",
            "style": "IPY_MODEL_5a4554bc8b6142d1b8691090e514ee43",
            "value": " 9598/9598 [1:21:22&lt;00:00,  1.97it/s, v_num=2]"
          }
        },
        "af09f294119840a480987887a400cb1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "ed6a11d6ec3d4609aa99c54960d8096a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05dc0e0ec8f44f72b22c36492a9d9ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46a9ffa311e048069270d4e424ba75c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1abc51e8c0bb4bc1bf893bddce805b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61fc685c4c4a437a863837e46cfe9683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4554bc8b6142d1b8691090e514ee43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a07c493878c4acba4ef680bec39fffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4cba80dbf0e4d1eb63ac871279e53e2",
              "IPY_MODEL_c9a65078a2384d81a5c5f0e67bb0d4ee",
              "IPY_MODEL_3466727fdb4742fc99c3fa0ea6180c98"
            ],
            "layout": "IPY_MODEL_79bf07b215e7468b9955cf76429ff22c"
          }
        },
        "d4cba80dbf0e4d1eb63ac871279e53e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b76cb1825b8482e80ef0619cec77890",
            "placeholder": "​",
            "style": "IPY_MODEL_2c405344852d44a48b455dd71c67d920",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c9a65078a2384d81a5c5f0e67bb0d4ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc3c8fb30bb44507bddcc66880d50be1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bf531ffe7e648aca1648bcbf428731d",
            "value": 2
          }
        },
        "3466727fdb4742fc99c3fa0ea6180c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8c0337e1973450897d8282fae7cfd48",
            "placeholder": "​",
            "style": "IPY_MODEL_4a50e363cb0146af9f7cc95f2e7ac9b1",
            "value": " 2/2 [00:15&lt;00:00,  7.21s/it]"
          }
        },
        "79bf07b215e7468b9955cf76429ff22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b76cb1825b8482e80ef0619cec77890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c405344852d44a48b455dd71c67d920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc3c8fb30bb44507bddcc66880d50be1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf531ffe7e648aca1648bcbf428731d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8c0337e1973450897d8282fae7cfd48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a50e363cb0146af9f7cc95f2e7ac9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will use an LLM on our dataset to perform the task of question-asnwerig. In particular we will use Llama-2-7b-chat-hf. There will be three main sections in which we will give zero examples (zero-shot), one (one-shot) and we will perform the fine tuning using our dataset to see the different performance. In the end there will be a comparison among all the models."
      ],
      "metadata": {
        "id": "jXC6gp_5jkM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8nORPeY4I96P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09690548-71c3-486a-b6b4-9677c7cecc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(f'/content/drive/MyDrive/Polimi/NLP')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0Ab2-9FKJSBa",
        "outputId": "2eefa726-eb66-4740-91d8-cebd32f38b0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Polimi/NLP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data upload and import libreries"
      ],
      "metadata": {
        "id": "EIwxoNLEjkM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"transformers\" \"peft\" \"accelerate\" \"bitsandbytes\" \"trl\" \"safetensors\" \"tiktoken\"\n",
        "!pip install -q -U langchain\n",
        "!pip install -q langchain-community langchain-core\n",
        "!pip install -q --no-deps peft\n",
        "!pip install -q lightning"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:40:42.308015Z",
          "iopub.execute_input": "2025-05-21T15:40:42.308255Z",
          "iopub.status.idle": "2025-05-21T15:42:01.003756Z",
          "shell.execute_reply.started": "2025-05-21T15:40:42.308232Z",
          "shell.execute_reply": "2025-05-21T15:42:01.003075Z"
        },
        "id": "aHpbbrYPjkM2"
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from peft import LoraConfig\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "from huggingface_hub import login\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:42:14.219133Z",
          "iopub.execute_input": "2025-05-21T15:42:14.219934Z",
          "iopub.status.idle": "2025-05-21T15:42:40.143610Z",
          "shell.execute_reply.started": "2025-05-21T15:42:14.219889Z",
          "shell.execute_reply": "2025-05-21T15:42:40.142846Z"
        },
        "id": "9VYs9WwYjkM3"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arnv53hIGISu",
        "outputId": "8edcb872-4999-4f3d-eed9-8569d6b07017"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load of the data"
      ],
      "metadata": {
        "id": "xb5FhLnxptNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this on colab\n",
        "train_set = pd.read_parquet(\"tuning_data.parquet\")\n",
        "test_set = pd.read_parquet(\"test_data.parquet\")"
      ],
      "metadata": {
        "id": "t39cd7KakixP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this on kaggle\n",
        "train_set=pd.read_parquet(\"/kaggle/input/nlp-resources/tuning_data.parquet\")\n",
        "test_set=pd.read_parquet(\"/kaggle/input/nlp-resources/test_data.parquet\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:42:40.145648Z",
          "iopub.execute_input": "2025-05-21T15:42:40.146204Z",
          "iopub.status.idle": "2025-05-21T15:42:40.615843Z",
          "shell.execute_reply.started": "2025-05-21T15:42:40.146183Z",
          "shell.execute_reply": "2025-05-21T15:42:40.615070Z"
        },
        "id": "5aatMmMSjkM4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing we will use a very small subset due to the fact that inference takes much time because of the large model."
      ],
      "metadata": {
        "id": "CGYPCFwsr0zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_set = test_set[:30]"
      ],
      "metadata": {
        "id": "u4sB3IPPMC1z"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"your_token\")"
      ],
      "metadata": {
        "id": "rmcwkAlovAMo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load of the model"
      ],
      "metadata": {
        "id": "1YCBJyW3jkM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the model and his tokenizer."
      ],
      "metadata": {
        "id": "XgSD7dL6jkM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:42:40.618877Z",
          "iopub.execute_input": "2025-05-21T15:42:40.619240Z",
          "iopub.status.idle": "2025-05-21T15:42:40.622507Z",
          "shell.execute_reply.started": "2025-05-21T15:42:40.619213Z",
          "shell.execute_reply": "2025-05-21T15:42:40.621906Z"
        },
        "id": "Ln3d5CbIjkM4"
      },
      "outputs": [],
      "execution_count": 46
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is too big, so we will make use of a 4-bit version of the model."
      ],
      "metadata": {
        "id": "-5BypDJjjkM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BitsAndBytesConfig int-4 config\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:42:40.623205Z",
          "iopub.execute_input": "2025-05-21T15:42:40.623415Z",
          "iopub.status.idle": "2025-05-21T15:42:40.635942Z",
          "shell.execute_reply.started": "2025-05-21T15:42:40.623401Z",
          "shell.execute_reply": "2025-05-21T15:42:40.635467Z"
        },
        "id": "u-P6LNLLjkM4"
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the model"
      ],
      "metadata": {
        "id": "nCWuIwdyp42F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:42:40.636621Z",
          "iopub.execute_input": "2025-05-21T15:42:40.636825Z",
          "iopub.status.idle": "2025-05-21T15:43:43.127434Z",
          "shell.execute_reply.started": "2025-05-21T15:42:40.636804Z",
          "shell.execute_reply": "2025-05-21T15:43:43.126876Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ebecf1fe5c5e4ae896043620561f2c0e",
            "e2d5e682805948deb31a0531d9eb7aba",
            "42442305382d4286971622e4bace4ac0",
            "54cafaa7813e43a7bdbbc301d8374212",
            "35c2664be18c48aa8dc44fafe84d2af2",
            "d7abaa74ac194ccf9199c82c90175a78",
            "25a33609a22744bdae3a0068c3f15900",
            "93f78eb4182c485dbca511bf47aaf516",
            "181c5543d3be41318833a2772753ef83",
            "d2a559cf5e6c4f1aaac3186ed93809a5",
            "b9e2e93f311b439da958b57cdcd3c1ef"
          ]
        },
        "id": "01r6OU-rjkM5",
        "outputId": "a839d5f1-2227-42a0-a298-1db980e5853f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebecf1fe5c5e4ae896043620561f2c0e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load of the tokenizer"
      ],
      "metadata": {
        "id": "dFnaJzC0p7o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:43.135091Z",
          "iopub.execute_input": "2025-05-21T15:43:43.135415Z",
          "iopub.status.idle": "2025-05-21T15:43:47.016615Z",
          "shell.execute_reply.started": "2025-05-21T15:43:43.135391Z",
          "shell.execute_reply": "2025-05-21T15:43:47.015765Z"
        },
        "id": "pozTU87xjkM5"
      },
      "outputs": [],
      "execution_count": 49
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:43.128100Z",
          "iopub.execute_input": "2025-05-21T15:43:43.128337Z",
          "iopub.status.idle": "2025-05-21T15:43:43.134410Z",
          "shell.execute_reply.started": "2025-05-21T15:43:43.128320Z",
          "shell.execute_reply": "2025-05-21T15:43:43.133745Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe5OZ4e0jkM5",
        "outputId": "3a1adf95-b26f-488a-e494-109d95628356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Generation Utilities"
      ],
      "metadata": {
        "id": "K-xRrGzTjkM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines the core utility functions used for answer generation and evaluation. It includes a text generation pipeline, normalization routines, and metrics for computing Exact Match, F1, BLEU, and semantic similarity scores. These functions will be reused in subsequent evaluation experiments across different settings (zero-shot, one-shot, fine-tuning)."
      ],
      "metadata": {
        "id": "lu6o3wrajkM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 3000,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.1,\n",
        "    \"do_sample\": True,\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:47.017504Z",
          "iopub.execute_input": "2025-05-21T15:43:47.017883Z",
          "iopub.status.idle": "2025-05-21T15:43:47.021687Z",
          "shell.execute_reply.started": "2025-05-21T15:43:47.017860Z",
          "shell.execute_reply": "2025-05-21T15:43:47.021042Z"
        },
        "id": "Rn2HiJf1jkM5"
      },
      "outputs": [],
      "execution_count": 86
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the pipeline"
      ],
      "metadata": {
        "id": "7qaKtcs-uQ2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:47.022571Z",
          "iopub.execute_input": "2025-05-21T15:43:47.022816Z",
          "iopub.status.idle": "2025-05-21T15:43:47.037099Z",
          "shell.execute_reply.started": "2025-05-21T15:43:47.022793Z",
          "shell.execute_reply": "2025-05-21T15:43:47.036612Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXQ5l1p2jkM5",
        "outputId": "25916e0a-ce7f-4319-a9aa-fecd6a027fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "execution_count": 87
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function returns the generated answer given the prompt. We will use it for zero and one shot model"
      ],
      "metadata": {
        "id": "UuWq-R8QuLmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(prompt):\n",
        "    output = qa_pipeline(prompt, **generation_args)\n",
        "    return output[0]['generated_text']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:47.037759Z",
          "iopub.execute_input": "2025-05-21T15:43:47.037943Z",
          "iopub.status.idle": "2025-05-21T15:43:47.048862Z",
          "shell.execute_reply.started": "2025-05-21T15:43:47.037928Z",
          "shell.execute_reply": "2025-05-21T15:43:47.048097Z"
        },
        "id": "cMQOHAKEjkM5"
      },
      "outputs": [],
      "execution_count": 88
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions compute evaluation metrics used to compare predicted answers with ground truth references.\n",
        "`normalize_answer` standardizes answers by removing punctuation, articles, and extra spaces.\n",
        "`exact_match_score` checks whether the normalized prediction matches the reference exactly.\n",
        "`f1_score` measures the overlap between tokens in the prediction and the reference using precision and recall.\n",
        "`bleu_score` calculates the BLEU score, a metric based on n-gram overlaps, with smoothing for short texts."
      ],
      "metadata": {
        "id": "527MArCUutCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s):\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def remove_punc(text):\n",
        "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    gt_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gt_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def bleu_score(prediction, ground_truth):\n",
        "    smoothie = SmoothingFunction().method4  # migliora BLEU su frasi corte\n",
        "    pred_tokens = nltk.word_tokenize(prediction.lower())\n",
        "    gt_tokens = nltk.word_tokenize(ground_truth.lower())\n",
        "    return sentence_bleu([gt_tokens], pred_tokens, smoothing_function=smoothie)\n"
      ],
      "metadata": {
        "id": "lO8wKM3tFqo7"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a SentenceTransformer model to compute sentence embeddings for the target and generated answers, enabling semantic similarity evaluation"
      ],
      "metadata": {
        "id": "4GVzGB8DuzNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:47.049557Z",
          "iopub.execute_input": "2025-05-21T15:43:47.049773Z",
          "iopub.status.idle": "2025-05-21T15:43:58.715767Z",
          "shell.execute_reply.started": "2025-05-21T15:43:47.049753Z",
          "shell.execute_reply": "2025-05-21T15:43:58.715191Z"
        },
        "id": "4F0-zjYMjkM6"
      },
      "outputs": [],
      "execution_count": 94
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function Given a single candidate answer and a target answer,\n",
        "return the semantic similarity score between them."
      ],
      "metadata": {
        "id": "23DKOAw6vLjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_score(candidate_answer, target_answer):\n",
        "    # Encode the target and the candidate\n",
        "    target_embedding = embedding_model.encode(target_answer, convert_to_tensor=True)\n",
        "    candidate_embedding = embedding_model.encode(candidate_answer, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = util.cos_sim(target_embedding, candidate_embedding).item()\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:58.716525Z",
          "iopub.execute_input": "2025-05-21T15:43:58.716779Z",
          "iopub.status.idle": "2025-05-21T15:43:58.721237Z",
          "shell.execute_reply.started": "2025-05-21T15:43:58.716758Z",
          "shell.execute_reply": "2025-05-21T15:43:58.720674Z"
        },
        "id": "b5GrTfv0jkM6"
      },
      "outputs": [],
      "execution_count": 95
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot performance"
      ],
      "metadata": {
        "id": "hpv-4JKNjkM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin with the zero-shot LLM to see how the model answer to the given task of question-answering without any example.\n"
      ],
      "metadata": {
        "id": "FTrwA9CrjkM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "ZjCevs0MFaOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will print the answer to the first 5 samples of the test set."
      ],
      "metadata": {
        "id": "KR8VS1gbFcUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "\n",
        "for i, row in test_set.head(n).reset_index(drop=True).iterrows():\n",
        "    question = row['question']\n",
        "    context = row['context']\n",
        "    target_answer = row['answer']\n",
        "\n",
        "    prompt = (\n",
        "        \"[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\n\"\n",
        "        f\"Question: {question}\\nContext: {context}\\nAnswer: [/INST]\"\n",
        "    )\n",
        "\n",
        "    # Generate the answer\n",
        "    result = chatbot(prompt)\n",
        "\n",
        "    # Output\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Question:\", question)\n",
        "    print(\"\\nTarget answer:\", target_answer)\n",
        "    print(\"\\nGenerated answer:\", result)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T14:56:34.699188Z",
          "iopub.execute_input": "2025-05-21T14:56:34.699472Z",
          "iopub.status.idle": "2025-05-21T14:58:17.292766Z",
          "shell.execute_reply.started": "2025-05-21T14:56:34.699451Z",
          "shell.execute_reply": "2025-05-21T14:58:17.292201Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia7loOm3jkM6",
        "outputId": "9e24915e-9c43-4af9-c6dd-0eb1375395ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Question: Who is the music director of the Quebec Symphony Orchestra?\n",
            "\n",
            "Target answer: The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n",
            "\n",
            "Generated answer:   The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n",
            "----------------------------------------\n",
            "Example 2:\n",
            "Question: Who were the four students of the University of Port Harcourt that were allegedly murdered?\n",
            "\n",
            "Target answer: The four students of the University of Port Harcourt that were allegedly murdered were Chiadika Lordson, Ugonna Kelechi Obusor, Mike Lloyd Toku and Tekena Elkanah.\n",
            "\n",
            "Generated answer:   The names of the four students of the University of Port Harcourt who were allegedly murdered are:\n",
            "\n",
            "1. Chiadika Lordson\n",
            "2. Ugonna Kelechi Obusor\n",
            "3. Mike Lloyd Toku\n",
            "4. Tekena Elkanah\n",
            "\n",
            "I hope this information helps. Let me know if you have any other questions.\n",
            "----------------------------------------\n",
            "Example 3:\n",
            "Question: What did Paul Wall offer to all U.S. Olympic Medalists?\n",
            "\n",
            "Target answer: Paul Wall wants to give free gold grills to all U.S. Olympic Medalists.\n",
            "\n",
            "Generated answer:   According to the articles listed, Paul Wall offered to give free gold grills to all U.S. Olympic medalists.\n",
            "----------------------------------------\n",
            "Example 4:\n",
            "Question: What are the main agricultural products that African countries export to the rest of the world?\n",
            "\n",
            "Target answer: African countries mainly export cocoa, edible fruit and nuts, coffee and tea, and vegetables to the rest of the world.\n",
            "\n",
            "Generated answer:   Thank you for the question! African countries export a variety of agricultural products to the rest of the world, including:\n",
            "\n",
            "1. Cocoa: Many African countries, such as Ghana, Côte d'Ivoire, and Cameroon, are major producers and exporters of cocoa beans, which are used to make chocolate.\n",
            "2. Edible fruit and nuts: Countries such as South Africa, Morocco, and Egypt are known for their production of edible fruit and nuts, including citrus fruits, avocados, and nuts like almonds and peanuts.\n",
            "3. Coffee and tea: Africa is home to some of the world's most famous coffee and tea producers, such as Ethiopia, Kenya, and South Africa.\n",
            "4. Vegetables: Many African countries produce a wide range of vegetables, including tomatoes, onions, peppers, and potatoes.\n",
            "\n",
            "The main agricultural importing countries of these exports are the United States, China, Germany, Netherlands, and the United Kingdom. However, African countries do not feature as major suppliers in any of these markets.\n",
            "\n",
            "To address the issue of importing food, African governments and leaders should focus on innovation in production technology, reducing the cost of inputs such as energy and fertilizers, managing changes in climatic conditions, providing knowledge transfer and capacity building, investing in agriculture and related infrastructure, and improving access to information. By doing so, Africa can reduce its reliance on importing food and instead become a major supplier of agricultural products to the rest of the world.\n",
            "----------------------------------------\n",
            "Example 5:\n",
            "Question: What is the main goal of the CHI 2011 workshop on large interactive displays in public urban contexts?\n",
            "\n",
            "Target answer: The main goal of this one-day CHI 2011 workshop is to cross-fertilize insights from different disciplines, to establish a more general understanding of large interactive displays in public urban contexts, and to develop an agenda for future research directions in this area.\n",
            "\n",
            "Generated answer:   The main goal of the CHI 2011 workshop on large interactive displays in public urban contexts is to provide a platform for researchers and practitioners from different disciplines to exchange insights and establish a general understanding of the potential and impact of large interactive displays in public urban settings. The workshop aims to explore approaches and insights from different disciplines, including art, architecture, design, HCI, media theory, and social science, to design and evaluate large interactive displays in urban life.\n",
            "\n",
            "The workshop will focus on three main topics:\n",
            "\n",
            "1. Beyond Playful Interaction: The workshop will explore how to design installations that can engage people's attention beyond the initial novelty effect and direct their interest towards the content.\n",
            "2. Interaction Models: The workshop will discuss how different interaction methods shape people's experience in urban spaces and how they can be evaluated.\n",
            "3. Evaluation Methods: The workshop will examine different quantitative and qualitative methods for evaluating the success of large interactive displays in urban spaces and how they can be applied in different progress stages.\n",
            "\n",
            "The workshop aims to cross-fertilize insights from different disciplines and establish an agenda for future research directions in this area. At least one author of each accepted position paper needs to register for the workshop and for one or more days of the CHI conference itself.\n",
            "\n",
            "The workshop organizers are Uta Hinrichs, Nina Valkanova, Kai Kuikkaniemi, Giulio Jacucci, and Sheelagh Carpendale. They are all experts in the field of human-computer interaction, interaction design, and ubiquitous computing, and have a strong background in designing and evaluating large interactive displays in public urban settings.\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "execution_count": 90
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "j8gmLiqVFd5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the performance of the zero-shot llm."
      ],
      "metadata": {
        "id": "OVglA6Lyrqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_answers_zero_shot(model, tokenizer, test_data):\n",
        "    exact_matches = []\n",
        "    f1_scores = []\n",
        "    bleu_scores=[]\n",
        "    cosine_similarity_scores=[]\n",
        "\n",
        "    for idx, row in test_data.iterrows():\n",
        "        question = row['question']\n",
        "        context = row['context']\n",
        "        target_answer = row['answer']\n",
        "\n",
        "        prompt = (\n",
        "            \"[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\n\"\n",
        "            f\"Question: {question}\\nContext: {context}\\nAnswer: [/INST]\"\n",
        "        )\n",
        "\n",
        "        pred_answer = chatbot(prompt)\n",
        "\n",
        "        em = exact_match_score(pred_answer, target_answer)\n",
        "        f1 = f1_score(pred_answer, target_answer)\n",
        "        bleu = bleu_score(pred_answer, target_answer)\n",
        "        cosine_similarity_score = compute_similarity_score(pred_answer, target_answer)\n",
        "\n",
        "        exact_matches.append(em)\n",
        "        f1_scores.append(f1)\n",
        "        bleu_scores.append(bleu)\n",
        "        cosine_similarity_scores.append(cosine_similarity_score)\n",
        "\n",
        "        print(\"em: \", em)\n",
        "        print(\"f1: \", f1)\n",
        "        print(\"bleu: \", bleu)\n",
        "        print(\"cosine_similarity_scores: \", cosine_similarity_score)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return (\n",
        "      sum(bleu_scores) / len(bleu_scores) * 100,\n",
        "      sum(exact_matches) / len(exact_matches) * 100,\n",
        "      sum(f1_scores) / len(f1_scores) * 100,\n",
        "      sum(cosine_similarity_scores) / len(cosine_similarity_scores) * 100\n",
        "    )"
      ],
      "metadata": {
        "id": "UcH8f_DBFhpH"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bleu_zero_shot, avg_em_zero_shot, avg_f1_zero_shot, cosine_similarity_score_zero_shot= eval_answers_zero_shot(model, tokenizer, small_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX0zAcK9JD6p",
        "outputId": "349b983d-8c15-447b-c1bb-441d61dcac2a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "em:  1\n",
            "f1:  1.0\n",
            "bleu:  1.0\n",
            "cosine_similarity_scores:  1.0000001192092896\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.6060606060606061\n",
            "bleu:  0.2567003823288495\n",
            "cosine_similarity_scores:  0.9050230979919434\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.8000000000000002\n",
            "bleu:  0.5454951299940093\n",
            "cosine_similarity_scores:  0.9453641176223755\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.13765182186234817\n",
            "bleu:  0.0391252795259626\n",
            "cosine_similarity_scores:  0.8833816051483154\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.25352112676056343\n",
            "bleu:  0.09276033301639343\n",
            "cosine_similarity_scores:  0.8959423899650574\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.29394812680115273\n",
            "bleu:  0.09436649561108859\n",
            "cosine_similarity_scores:  0.8304895162582397\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.06837606837606837\n",
            "bleu:  0.005112042726386534\n",
            "cosine_similarity_scores:  0.02513803541660309\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.5862068965517242\n",
            "bleu:  0.17587339605122757\n",
            "cosine_similarity_scores:  0.8722813129425049\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.23913043478260873\n",
            "bleu:  0.06823516092133534\n",
            "cosine_similarity_scores:  0.651279628276825\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3468634686346863\n",
            "bleu:  0.06677912574873161\n",
            "cosine_similarity_scores:  0.8113175630569458\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.21428571428571425\n",
            "bleu:  0.08474571313664093\n",
            "cosine_similarity_scores:  0.8048169016838074\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.07329842931937172\n",
            "bleu:  0.032159467527803916\n",
            "cosine_similarity_scores:  0.5695435404777527\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.5426356589147286\n",
            "bleu:  0.20607034687960338\n",
            "cosine_similarity_scores:  0.949464738368988\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.9600000000000001\n",
            "bleu:  0.8843946454355334\n",
            "cosine_similarity_scores:  0.9863824844360352\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.2331288343558282\n",
            "bleu:  0.03275505410251452\n",
            "cosine_similarity_scores:  0.7999657392501831\n",
            "----------------------------------------\n",
            "em:  1\n",
            "f1:  1.0\n",
            "bleu:  0.5997820163128024\n",
            "cosine_similarity_scores:  0.9943488836288452\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.23423423423423423\n",
            "bleu:  0.0721171279327379\n",
            "cosine_similarity_scores:  0.4262518882751465\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.36363636363636365\n",
            "bleu:  0.14892064391622636\n",
            "cosine_similarity_scores:  0.6421036720275879\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.13617021276595745\n",
            "bleu:  0.035994202701158336\n",
            "cosine_similarity_scores:  0.8603610992431641\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.2554347826086956\n",
            "bleu:  0.06895773654134751\n",
            "cosine_similarity_scores:  0.8847994804382324\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.04098360655737705\n",
            "bleu:  0.009736482177639099\n",
            "cosine_similarity_scores:  0.5309112071990967\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.15\n",
            "bleu:  0.06115100780166429\n",
            "cosine_similarity_scores:  0.8655468225479126\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.1708185053380783\n",
            "bleu:  0.048420606972806306\n",
            "cosine_similarity_scores:  0.8233729600906372\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.2366412213740458\n",
            "bleu:  0.05244265143221813\n",
            "cosine_similarity_scores:  0.8923234343528748\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.1721311475409836\n",
            "bleu:  0.05198688873296149\n",
            "cosine_similarity_scores:  0.7447935342788696\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.29508196721311475\n",
            "bleu:  0.10733866744977241\n",
            "cosine_similarity_scores:  0.8379015326499939\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.525\n",
            "bleu:  0.2270746935679019\n",
            "cosine_similarity_scores:  0.9079351425170898\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.16326530612244897\n",
            "bleu:  0.012876137570887116\n",
            "cosine_similarity_scores:  0.8100685477256775\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.574585635359116\n",
            "bleu:  0.33167285275369635\n",
            "cosine_similarity_scores:  0.9136569499969482\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.40963855421686746\n",
            "bleu:  0.2553768981504627\n",
            "cosine_similarity_scores:  0.8732147812843323\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average BLEU score: {avg_bleu_zero_shot:.2f}\")\n",
        "print(f\"Exact Match: {avg_em_zero_shot:.2f}%\")\n",
        "print(f\"F1 Score: {avg_f1_zero_shot:.2f}%\")\n",
        "print(f\"Cosine Similarity Score: {cosine_similarity_score_zero_shot:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmhry8QGJ4FX",
        "outputId": "a0610dca-1e07-4308-ccec-9a2430a3849e"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 18.89\n",
            "Exact Match: 6.67%\n",
            "F1 Score: 36.94%\n",
            "Cosine Similarity Score: 79.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-shot performance"
      ],
      "metadata": {
        "id": "XGFaZXwEjkM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will give to the model a single example in addition with the task and then it will answer to a few question from the test set."
      ],
      "metadata": {
        "id": "DXLp-nIKjkM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Prompt template matching LLaMA-2 chat format\n",
        "llama_prompt = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful assistant.\n",
        "<</SYS>>\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "[/INST]\n",
        "{answer}\"\"\"\n",
        "\n",
        "# We use this function to format the llama_prompt correctly\n",
        "def format_prompt(context, question, answer=None):\n",
        "    # If answer is None, prepare prompt for generation (no answer)\n",
        "    if answer is None:\n",
        "        answer = \"\"\n",
        "    return llama_prompt.format(context=context, question=question, answer=answer) + EOS_TOKEN"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:58.723516Z",
          "iopub.execute_input": "2025-05-21T15:43:58.723691Z",
          "iopub.status.idle": "2025-05-21T15:43:58.737859Z",
          "shell.execute_reply.started": "2025-05-21T15:43:58.723677Z",
          "shell.execute_reply": "2025-05-21T15:43:58.737256Z"
        },
        "id": "E4j6DWeqjkM7"
      },
      "outputs": [],
      "execution_count": 123
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare the prompt containing the example given to the llm."
      ],
      "metadata": {
        "id": "wyq2OysfslyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_shot_example = train_set.iloc[0]\n",
        "one_shot_prompt = format_prompt(\n",
        "    one_shot_example[\"context\"],\n",
        "    one_shot_example[\"question\"],\n",
        "    one_shot_example[\"answer\"]\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:58.738559Z",
          "iopub.execute_input": "2025-05-21T15:43:58.739009Z",
          "iopub.status.idle": "2025-05-21T15:43:58.753901Z",
          "shell.execute_reply.started": "2025-05-21T15:43:58.738992Z",
          "shell.execute_reply": "2025-05-21T15:43:58.753390Z"
        },
        "id": "upcSYmS6jkM7"
      },
      "outputs": [],
      "execution_count": 124
    },
    {
      "cell_type": "code",
      "source": [
        "print(one_shot_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpkKKPv-_S4c",
        "outputId": "80434be0-2c24-40aa-f877-815c3a38c54d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] <<SYS>>\n",
            "You are a helpful assistant.\n",
            "<</SYS>>\n",
            "\n",
            "Context: Caption: Tasmanian berry grower Nic Hansen showing Macau chef Antimo Merone around his property as part of export engagement activities.\n",
            "THE RISE and rise of the Australian strawberry, raspberry and blackberry industries has seen the sectors redouble their international trade focus, with the release of a dedicated export plan to grow their global presence over the next 10 years.\n",
            "Driven by significant grower input, the Berry Export Summary 2028 maps the sectors’ current position, where they want to be, high-opportunity markets and next steps.\n",
            "Hort Innovation trade manager Jenny Van de Meeberg said the value and volume of raspberry and blackberry exports rose by 100 per cent between 2016 and 2017. She said the Australian strawberry industry experienced similar success with an almost 30 per cent rise in export volume and a 26 per cent rise in value to $32.6M over the same period.\n",
            "“Australian berry sectors are in a firm position at the moment,” she said. “Production, adoption of protected substrate cropping, improved genetics and an expanding geographic footprint have all helped put Aussie berries on a positive trajectory.\n",
            "“We are seeing a real transition point. Broad industry interest and a strong commercial appetite for export market development combined with the potential to capitalise on existing trade agreements and build new trade partnerships has created this perfect environment for growth.”\n",
            "High-income countries across Europe, North America and Northern Asia have been identified as having a palate for Australian grown berries with more than 4244 tonnes of fresh berries exported in the last financial year alone.\n",
            "The strategy identified the best short-term prospect markets for the Australian blackberry and raspberry industry as Hong Kong, Singapore, The United Arab Emirates and Canada. The strongest short-term trade options identified for the strawberry sector were Thailand, Malaysia, New Zealand and Macau.\n",
            "The strategy focuses heavily on growing the existing strawberry export market from 4 per cent to at least 8 per cent of national production by volume, in markets with a capacity and willingness to pay a premium for quality fruit. For raspberries and blackberries, the sectors aim to achieve a 5 per cent boost in exports assessed by volume across identified markets by 2021.\n",
            "Tasmanian raspberry exporter Nic Hansen said Australia offers some of the sweetest and most attractive berries in the world, and this combined with our stringent food safety standards across all stages of the supply chain puts growers in a solid position.\n",
            "“We have a great product, we are hungry to expand trade and now with this new plan in place, we have a clear roadmap towards driving growth,” Mr Hansen said.\n",
            "He said it is exciting to see new export market prospects for raspberries: “The more options we have for export the better. Now we just have to get on with the job of ensuring industry has all the tools it needs, such as supporting data and relationship building opportunities, to thrive in new markets.”\n",
            "This project was commissioned by Hort Innovation, and developed by market analysts and research consultants Auspex Strategic Advisory and AgInfinity. Hort Innovation will work now with berry sectors to determine levy-funded activities to support trade.\n",
            "See a summary of the strategy on the Hort Innovation website.\n",
            "For more information on the berry industries, refer to the Horticulture Statistics Handbook and the Strategic Investment Plans for strawberries, raspberries and blackberries. Growers seeking more information should email trade@horticulture.com.au\n",
            "\n",
            "Question: What is the Berry Export Summary 2028 and what is its purpose?\n",
            "[/INST]\n",
            "The Berry Export Summary 2028 is a dedicated export plan for the Australian strawberry, raspberry, and blackberry industries. It maps the sectors’ current position, where they want to be, high-opportunity markets, and next steps. The purpose of this plan is to grow their global presence over the next 10 years.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "PFESWXqlMgP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test the one-shot model"
      ],
      "metadata": {
        "id": "1_xX9EiBtbnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "\n",
        "for i, row in test_set.head(n).reset_index(drop=True).iterrows():\n",
        "    question = row['question']\n",
        "    context = row['context']\n",
        "    target_answer = row['answer']\n",
        "\n",
        "    # Format current test prompt WITHOUT answer\n",
        "    test_prompt = format_prompt(context, question, answer=None)\n",
        "\n",
        "    # Combine one-shot example + current test prompt\n",
        "    full_prompt = one_shot_prompt + \"\\n\" + test_prompt\n",
        "\n",
        "    # Generate answer using your chatbot function adapted to receive full_prompt\n",
        "    result = chatbot(full_prompt)  # Pass full prompt, not just context+question\n",
        "\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Question:\", question)\n",
        "    print(\"\\nTarget answer:\", target_answer)\n",
        "    print(\"\\nGenerated answer:\", result)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T15:43:58.754516Z",
          "iopub.execute_input": "2025-05-21T15:43:58.754763Z",
          "iopub.status.idle": "2025-05-21T15:45:03.674447Z",
          "shell.execute_reply.started": "2025-05-21T15:43:58.754745Z",
          "shell.execute_reply": "2025-05-21T15:45:03.673816Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMCQdoHfjkM7",
        "outputId": "eafff674-9c85-4795-fcbe-58a96018fcdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Question: Who is the music director of the Quebec Symphony Orchestra?\n",
            "\n",
            "Target answer: The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n",
            "\n",
            "Generated answer: The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n",
            "----------------------------------------\n",
            "Example 2:\n",
            "Question: Who were the four students of the University of Port Harcourt that were allegedly murdered?\n",
            "\n",
            "Target answer: The four students of the University of Port Harcourt that were allegedly murdered were Chiadika Lordson, Ugonna Kelechi Obusor, Mike Lloyd Toku and Tekena Elkanah.\n",
            "\n",
            "Generated answer: According to the article, the four students of the University of Port Harcourt who were allegedly murdered are:\n",
            "\n",
            "1. Chiadika Lordson\n",
            "2. Ugonna Kelechi Obusor\n",
            "3. Mike Lloyd Toku\n",
            "4. Tekena Elkanah.\n",
            "----------------------------------------\n",
            "Example 3:\n",
            "Question: What did Paul Wall offer to all U.S. Olympic Medalists?\n",
            "\n",
            "Target answer: Paul Wall wants to give free gold grills to all U.S. Olympic Medalists.\n",
            "\n",
            "Generated answer: According to the article, Paul Wall offered to give free gold grills to all U.S. Olympic medalists.\n",
            "----------------------------------------\n",
            "Example 4:\n",
            "Question: What are the main agricultural products that African countries export to the rest of the world?\n",
            "\n",
            "Target answer: African countries mainly export cocoa, edible fruit and nuts, coffee and tea, and vegetables to the rest of the world.\n",
            "\n",
            "Generated answer: The main agricultural products that African countries export to the rest of the world include:\n",
            "\n",
            "1. Cocoa\n",
            "2. Edible fruit and nuts\n",
            "3. Coffee\n",
            "4. Tea\n",
            "5. Vegetables\n",
            "\n",
            "African countries mainly export these products to the United States, China, Germany, Netherlands, and the United Kingdom. However, they do not feature as top supplying countries for any of these markets. To change from agricultural product importers, addressing issues such as innovation in production technology, the cost of inputs, management of changes in climatic conditions, knowledge transfer and capacity building, investment in agriculture and agriculture-related infrastructure, and access to information is crucial.\n",
            "----------------------------------------\n",
            "Example 5:\n",
            "Question: What is the main goal of the CHI 2011 workshop on large interactive displays in public urban contexts?\n",
            "\n",
            "Target answer: The main goal of this one-day CHI 2011 workshop is to cross-fertilize insights from different disciplines, to establish a more general understanding of large interactive displays in public urban contexts, and to develop an agenda for future research directions in this area.\n",
            "\n",
            "Generated answer: The main goal of the CHI 2011 workshop on large interactive displays in public urban contexts is to provide a platform for researchers and practitioners from different disciplines to exchange insights and establish a more general understanding of large interactive displays in public urban settings. The workshop aims to cross-fertilize ideas and develop an agenda for future research directions in this area, with a focus on the following topics:\n",
            "\n",
            "1. Beyond Playful Interaction: How can we design installations that engage people's attention beyond the initial novelty effect and direct their interest towards the content?\n",
            "2. Interaction Models: How do different interaction methods shape people's experience of large display installations in urban spaces?\n",
            "3. Evaluation Methods: How can we evaluate the success of large display installations in urban spaces, and what are the most effective evaluation methods in different progress stages (design phase/installation phase)?\n",
            "\n",
            "The workshop organizers welcome submissions from various disciplines, including art, architecture, design, HCI, media theory, and social science, and encourage all methodological approaches and techniques centered around the topic of large interactive displays in urban life.\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "execution_count": 60
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "QvTbKJDPMi2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the  one-shot model"
      ],
      "metadata": {
        "id": "coujQsEot6Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_answers_one_shot(model, tokenizer, test_data):\n",
        "    exact_matches = []\n",
        "    f1_scores = []\n",
        "    bleu_scores=[]\n",
        "    cosine_similarity_scores=[]\n",
        "\n",
        "    for idx, row in test_data.iterrows():\n",
        "        question = row['question']\n",
        "        context = row['context']\n",
        "        target_answer = row['answer']\n",
        "\n",
        "        # Format current test prompt WITHOUT answer\n",
        "        test_prompt = format_prompt(context, question, answer=None)\n",
        "\n",
        "        # Combine one-shot example + current test prompt\n",
        "        full_prompt = one_shot_prompt + \"\\n\" + test_prompt\n",
        "\n",
        "        pred_answer = chatbot(full_prompt)\n",
        "\n",
        "        em = exact_match_score(pred_answer, target_answer)\n",
        "        f1 = f1_score(pred_answer, target_answer)\n",
        "        bleu = bleu_score(pred_answer, target_answer)\n",
        "        cosine_similarity_score = compute_similarity_score(pred_answer, target_answer)\n",
        "\n",
        "        exact_matches.append(em)\n",
        "        f1_scores.append(f1)\n",
        "        bleu_scores.append(bleu)\n",
        "        cosine_similarity_scores.append(cosine_similarity_score)\n",
        "\n",
        "        print(\"em: \", em)\n",
        "        print(\"f1: \", f1)\n",
        "        print(\"bleu: \", bleu)\n",
        "        print(\"cosine_similarity_scores: \", cosine_similarity_score)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return (\n",
        "      sum(bleu_scores) / len(bleu_scores) * 100,\n",
        "      sum(exact_matches) / len(exact_matches) * 100,\n",
        "      sum(f1_scores) / len(f1_scores) * 100,\n",
        "      sum(cosine_similarity_scores) / len(cosine_similarity_scores) * 100\n",
        "    )"
      ],
      "metadata": {
        "id": "mSEfPe_rXoyo"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bleu_one_shot, avg_em_one_shot, avg_f1_one_shot, cosine_similarity_score_one_shot= eval_answers_one_shot(model, tokenizer, small_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c79cda-fca1-449f-efe7-762eb448b370",
        "id": "4LDygXqIXoyp"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "em:  1\n",
            "f1:  1.0\n",
            "bleu:  1.0\n",
            "cosine_similarity_scores:  1.0000001192092896\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.7692307692307693\n",
            "bleu:  0.38260294162784475\n",
            "cosine_similarity_scores:  0.9396244883537292\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.8275862068965517\n",
            "bleu:  0.5767908748024404\n",
            "cosine_similarity_scores:  0.9525476694107056\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.32142857142857145\n",
            "bleu:  0.09128266909356\n",
            "cosine_similarity_scores:  0.8619110584259033\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3681592039800995\n",
            "bleu:  0.14125123814566634\n",
            "cosine_similarity_scores:  0.9050204753875732\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.29041095890410956\n",
            "bleu:  0.08695607321230917\n",
            "cosine_similarity_scores:  0.8855873942375183\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.6126126126126126\n",
            "bleu:  0.3536744074148674\n",
            "cosine_similarity_scores:  0.7865282893180847\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.8947368421052632\n",
            "bleu:  0.6480785272594485\n",
            "cosine_similarity_scores:  0.9337969422340393\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.21641791044776118\n",
            "bleu:  0.05838621225309788\n",
            "cosine_similarity_scores:  0.6671164035797119\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.5026178010471205\n",
            "bleu:  0.08586614454444348\n",
            "cosine_similarity_scores:  0.8995440602302551\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.19354838709677416\n",
            "bleu:  0.07697049512509863\n",
            "cosine_similarity_scores:  0.7613838911056519\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.4583333333333333\n",
            "bleu:  0.1309822409175701\n",
            "cosine_similarity_scores:  0.9056893587112427\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3947368421052631\n",
            "bleu:  0.17094473590478637\n",
            "cosine_similarity_scores:  0.8854309320449829\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.9600000000000001\n",
            "bleu:  0.8843946454355334\n",
            "cosine_similarity_scores:  0.9863824844360352\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3863636363636363\n",
            "bleu:  0.1095926543900107\n",
            "cosine_similarity_scores:  0.8432915210723877\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.923076923076923\n",
            "bleu:  0.4790714250659131\n",
            "cosine_similarity_scores:  0.9774834513664246\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.6842105263157895\n",
            "bleu:  0.6190034481596325\n",
            "cosine_similarity_scores:  0.6451582312583923\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.4313725490196079\n",
            "bleu:  0.21632781562727707\n",
            "cosine_similarity_scores:  0.6080496907234192\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.23188405797101452\n",
            "bleu:  0.05056574009763546\n",
            "cosine_similarity_scores:  0.8815410733222961\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3254237288135593\n",
            "bleu:  0.11150302426142165\n",
            "cosine_similarity_scores:  0.9287835955619812\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.13043478260869565\n",
            "bleu:  0.025454980318328303\n",
            "cosine_similarity_scores:  0.7613410353660583\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.4523809523809524\n",
            "bleu:  0.2745922342317162\n",
            "cosine_similarity_scores:  0.9153560400009155\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.45000000000000007\n",
            "bleu:  0.13565827594355806\n",
            "cosine_similarity_scores:  0.8049706220626831\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.24342105263157895\n",
            "bleu:  0.06298530157468872\n",
            "cosine_similarity_scores:  0.7825706005096436\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3492063492063492\n",
            "bleu:  0.15799556916793744\n",
            "cosine_similarity_scores:  0.8381270170211792\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3\n",
            "bleu:  0.10025421692498239\n",
            "cosine_similarity_scores:  0.928337812423706\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.5185185185185185\n",
            "bleu:  0.18869321621980917\n",
            "cosine_similarity_scores:  0.8814116716384888\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.19626168224299065\n",
            "bleu:  0.02166575898165109\n",
            "cosine_similarity_scores:  0.8084359169006348\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.5221674876847291\n",
            "bleu:  0.30445629522728723\n",
            "cosine_similarity_scores:  0.8632658123970032\n",
            "----------------------------------------\n",
            "em:  1\n",
            "f1:  1.0\n",
            "bleu:  1.0\n",
            "cosine_similarity_scores:  1.0000001192092896\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average BLEU score: {avg_bleu_one_shot:.2f}\")\n",
        "print(f\"Exact Match: {avg_em_one_shot:.2f}%\")\n",
        "print(f\"F1 Score: {avg_f1_one_shot:.2f}%\")\n",
        "print(f\"Cosine Similarity Score: {cosine_similarity_score_one_shot:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc83a122-cc80-4823-f756-5dd3ed018491",
        "id": "7f_Fo9RVXoyp"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 28.49\n",
            "Exact Match: 6.67%\n",
            "F1 Score: 49.85%\n",
            "Cosine Similarity Score: 86.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning"
      ],
      "metadata": {
        "id": "qYs7040FjkM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will try to fine tune the Llama-2-7b-chat-hf llm using our dataset to see if there are any improvement in the question-answering."
      ],
      "metadata": {
        "id": "rgqfcphKI3gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of the model"
      ],
      "metadata": {
        "id": "QNOHZLpZGnMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the LoRA adapter due to the fact that the model is very big and we wouldn't be able to train it."
      ],
      "metadata": {
        "id": "kY1ra62cjkM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_configs = {\n",
        "    'target_modules': 'all-linear',\n",
        "    'lora_alpha': 16,\n",
        "    'lora_dropout': 0.1,\n",
        "    'r': 16,\n",
        "    'bias': 'none',\n",
        "    'task_type': 'CAUSAL_LM'\n",
        "}\n",
        "\n",
        "lora_configs = LoraConfig(**adapter_configs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T14:33:42.873865Z",
          "iopub.execute_input": "2025-05-21T14:33:42.874149Z",
          "iopub.status.idle": "2025-05-21T14:33:42.878092Z",
          "shell.execute_reply.started": "2025-05-21T14:33:42.874129Z",
          "shell.execute_reply": "2025-05-21T14:33:42.877385Z"
        },
        "id": "1DxyfBUNjkM_"
      },
      "outputs": [],
      "execution_count": 104
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model for 4-bit quantized training and apply LoRA (Low-Rank Adaptation) using the specified configuration"
      ],
      "metadata": {
        "id": "gyh7Bd1mvUBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_model_4bit = prepare_model_for_kbit_training(model)\n",
        "model2 = get_peft_model(prepared_model_4bit, lora_configs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T14:33:45.292974Z",
          "iopub.execute_input": "2025-05-21T14:33:45.293264Z",
          "iopub.status.idle": "2025-05-21T14:33:46.045886Z",
          "shell.execute_reply.started": "2025-05-21T14:33:45.293243Z",
          "shell.execute_reply": "2025-05-21T14:33:46.045247Z"
        },
        "id": "rbUVCLDljkM_"
      },
      "outputs": [],
      "execution_count": 105
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt format for LLaMA-2 chat models\n",
        "llama_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Format the dataset examples into the appropriate prompt format required by the LLaMA model.\n",
        "# Each prompt includes the question, context, and expected answer, and ends with an EOS token to prevent infinite generation.\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"question\"]\n",
        "    inputs       = examples[\"context\"]\n",
        "    outputs      = examples[\"answer\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = llama_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_set)\n",
        "formatted_train_set = train_dataset .map(formatting_prompts_func, batched=True,)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "010209eeabf3408280dc1c4ff8a65bd0",
            "b373cd61c4f245fc861c2824d9b12be3",
            "fec45749adaf444e9389e5fc046cc19e",
            "c8a2b2aee7164d0eacda248b5c672dd8",
            "28ec92afff844b3f9b9dc8a1c926fff5",
            "ddaaf535e4b140a98fdede358c87f8d0",
            "a2c76874c54b44c49aff6c2a03fd6446",
            "26aaeea44d7c4b57b252e11d4bdb198a",
            "5542ba2898764e4488875bc49ca071c7",
            "6b8b056919a94d6e8a375bac426b4167",
            "c6ccfba572a74d2f874ccabda384fd66"
          ]
        },
        "id": "r60zu_ZVjkM_",
        "outputId": "8429c9b3-6256-4313-ab23-fe6d2d053e16"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "010209eeabf3408280dc1c4ff8a65bd0"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 137
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrap the model with a PyTorch Lightning module"
      ],
      "metadata": {
        "id": "3kVIZtTQvtUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningWrapper(L.LightningModule):\n",
        "    def __init__(self, model, tokeniser, lr=1.e-4):\n",
        "        super().__init__()\n",
        "        self._model = model\n",
        "        self._tokeniser = tokeniser\n",
        "        self._lr = lr\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Build optimiser\n",
        "        optimiser = AdamW(self.parameters(), lr=self._lr)\n",
        "\n",
        "        return optimiser\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self._model.forward(*args, **kwargs)\n",
        "\n",
        "    def training_step(self, mini_batch, mini_batch_idx):\n",
        "        # Unpack the encoding and the target labels\n",
        "        input_encodings, labels = mini_batch\n",
        "        # Run generic forward step\n",
        "        output = self.forward(**input_encodings)\n",
        "        # Compute logits\n",
        "        logits: torch.tensor = output.logits\n",
        "        # Shift logits to exclude the last element\n",
        "        logits = logits[..., :-1, :].contiguous()\n",
        "        # shift labels to exclude the first element\n",
        "        labels = labels[..., 1:].contiguous()\n",
        "        # Compute LM loss token-wise\n",
        "        loss: torch.tensor = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "s_u2wdfpothy"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "PVVNADvSGriQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train our model."
      ],
      "metadata": {
        "id": "E89HdamXHGef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare training data loader\n"
      ],
      "metadata": {
        "id": "P2jVqakyv0aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def collate(mini_batch):\n",
        "    input_encodings = tokenizer([sample['text'] for sample in mini_batch], return_tensors='pt', padding=True)\n",
        "    labels = input_encodings.input_ids.clone()\n",
        "    labels[~input_encodings.attention_mask.bool()] = -100\n",
        "\n",
        "    return input_encodings, labels\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    formatted_train_set, collate_fn=collate, shuffle=True, batch_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "gBB_k86woMLU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lightning_model = LightningWrapper(model2, tokenizer)"
      ],
      "metadata": {
        "id": "W0fzkd-dE75q"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the training setup with gradient accumulation, mixed precision, and gradient clipping.\n",
        "This helps improve training efficiency, stability, and performance."
      ],
      "metadata": {
        "id": "RnjSWKp7wQn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = L.Trainer(\n",
        "    accumulate_grad_batches=32,\n",
        "    precision='bf16-mixed',  # Mixed precision (bf16-mixed or 16-mixed)\n",
        "    gradient_clip_val=1.0,  # Gradient clipping\n",
        "    max_epochs=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5AbOA4pnUx6",
        "outputId": "08f780b8-f277-4e8b-aaf8-1479a6dd57b7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the training"
      ],
      "metadata": {
        "id": "HMe4wJyNwUwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(lightning_model, train_dataloaders=data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572,
          "referenced_widgets": [
            "06fe9787c4b2487a92ca44eff04686ea",
            "e3f1e2ec3b3e460d86776811d4eea493",
            "52a8cecaaf964698acf2b85574c6d36c",
            "85c97de3ae644fae8ba2bc32e49cba2d",
            "af09f294119840a480987887a400cb1f",
            "ed6a11d6ec3d4609aa99c54960d8096a",
            "05dc0e0ec8f44f72b22c36492a9d9ce9",
            "46a9ffa311e048069270d4e424ba75c8",
            "1abc51e8c0bb4bc1bf893bddce805b60",
            "61fc685c4c4a437a863837e46cfe9683",
            "5a4554bc8b6142d1b8691090e514ee43"
          ]
        },
        "id": "-pVl1235neqb",
        "outputId": "131edabc-b3f9-4783-f4b5-d45eb43c8f24"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name   | Type                 | Params | Mode \n",
            "--------------------------------------------------------\n",
            "0 | _model | PeftModelForCausalLM | 3.5 B  | train\n",
            "--------------------------------------------------------\n",
            "40.0 M    Trainable params\n",
            "3.5 B     Non-trainable params\n",
            "3.5 B     Total params\n",
            "14,161.560Total estimated model params size (MB)\n",
            "2242      Modules in train mode\n",
            "423       Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name   | Type                 | Params | Mode \n",
            "--------------------------------------------------------\n",
            "0 | _model | PeftModelForCausalLM | 3.5 B  | train\n",
            "--------------------------------------------------------\n",
            "40.0 M    Trainable params\n",
            "3.5 B     Non-trainable params\n",
            "3.5 B     Total params\n",
            "14,161.560Total estimated model params size (MB)\n",
            "2242      Modules in train mode\n",
            "423       Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06fe9787c4b2487a92ca44eff04686ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the weights."
      ],
      "metadata": {
        "id": "SX05BdYpJ7uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(lightning_model.state_dict(), 'model2_Llama.pth')"
      ],
      "metadata": {
        "id": "JElu0ZQaoIAj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_checkpoint(\"model2_Llama.ckpt\")"
      ],
      "metadata": {
        "id": "MpJu9z4EDZaS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "PTRvXjdpLdaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the fine tuned model and see if there are any improvements."
      ],
      "metadata": {
        "id": "Bq09w0f-M5z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load first the model with the weights obtained from the fine tuning."
      ],
      "metadata": {
        "id": "H8Tpr85tNBFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args_fine_tuned = {\n",
        "    \"max_new_tokens\": 3000,\n",
        "    \"temperature\": 0.1,\n",
        "    \"do_sample\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "QXwAcKYrb8Kk"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model."
      ],
      "metadata": {
        "id": "-0PWZN7Mwsvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1a07c493878c4acba4ef680bec39fffb",
            "d4cba80dbf0e4d1eb63ac871279e53e2",
            "c9a65078a2384d81a5c5f0e67bb0d4ee",
            "3466727fdb4742fc99c3fa0ea6180c98",
            "79bf07b215e7468b9955cf76429ff22c",
            "1b76cb1825b8482e80ef0619cec77890",
            "2c405344852d44a48b455dd71c67d920",
            "cc3c8fb30bb44507bddcc66880d50be1",
            "2bf531ffe7e648aca1648bcbf428731d",
            "f8c0337e1973450897d8282fae7cfd48",
            "4a50e363cb0146af9f7cc95f2e7ac9b1"
          ]
        },
        "id": "rVR5mxZfEJfx",
        "outputId": "f4236313-cf89-4416-d323-0a9271061f2a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a07c493878c4acba4ef680bec39fffb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the weights in the model"
      ],
      "metadata": {
        "id": "3UHw4t3swv13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(\"model2_Llama.pth\", map_location=\"cuda\")\n",
        "fine_tuned_model.load_state_dict(state_dict, strict=False)\n",
        "fine_tuned_model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzmx-XmlzJHp",
        "outputId": "dc488cde-2a07-4efd-bc6e-7b9bc40bd801"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function that generates and returns the answer.The genereted answer may include not only the direct answer but also additional context,\n",
        "explanations, or formatting."
      ],
      "metadata": {
        "id": "kv26Rfskw1kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_fine_tuned(prompt, tokenizer, model, generation_args):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate output\n",
        "    outputs = model.generate(**inputs, **generation_args_fine_tuned)\n",
        "\n",
        "    # Decode generated tokens\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "WeTl-j-aMrL5"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function that extracts only the answer to the question from the generated response."
      ],
      "metadata": {
        "id": "zbbnji8qw8Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer_only(generated_text):\n",
        "    # Cerca il delimitatore \"### Response:\" e restituisce solo ciò che viene dopo\n",
        "    if \"### Response:\" in generated_text:\n",
        "        return generated_text.split(\"### Response:\")[1].strip()\n",
        "    elif \"[/INST]\" in generated_text:\n",
        "        return generated_text.split(\"[/INST]\")[-1].strip()\n",
        "    else:\n",
        "        return generated_text.strip()"
      ],
      "metadata": {
        "id": "YDI7Xjl_2N6g"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on few samples."
      ],
      "metadata": {
        "id": "aXNW_tlqxmA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "\n",
        "for i, row in test_set.head(n).reset_index(drop=True).iterrows():\n",
        "    question = row['question']\n",
        "    context = row['context']\n",
        "    target_answer = row['answer']\n",
        "\n",
        "    prompt = llama_prompt.format(question, context, \"\")\n",
        "\n",
        "    # Generate the answer\n",
        "    result = chatbot_fine_tuned(prompt, tokenizer, fine_tuned_model, generation_args_fine_tuned)\n",
        "    clean_answer = extract_answer_only(result)\n",
        "\n",
        "    # Output\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Question:\", question)\n",
        "    print(\"\\nTarget answer:\", target_answer)\n",
        "    print(\"\\nGenerated answer:\", clean_answer)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oth1dmXgLgVj",
        "outputId": "726f8910-8809-4672-9173-c2dc7469ce5b"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Question: Who is the music director of the Quebec Symphony Orchestra?\n",
            "\n",
            "Target answer: The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n",
            "\n",
            "Generated answer: Fabien Gabel is the music director of the Quebec Symphony Orchestra.\n",
            "----------------------------------------\n",
            "Example 2:\n",
            "Question: Who were the four students of the University of Port Harcourt that were allegedly murdered?\n",
            "\n",
            "Target answer: The four students of the University of Port Harcourt that were allegedly murdered were Chiadika Lordson, Ugonna Kelechi Obusor, Mike Lloyd Toku and Tekena Elkanah.\n",
            "\n",
            "Generated answer: The four students of the University of Port Harcourt who were allegedly murdered are:\n",
            "\n",
            "1. Chiadika Lordson\n",
            "2. Ugonna Kelechi Obusor\n",
            "3. Mike Lloyd Toku\n",
            "4. Tekena Elkanah\n",
            "----------------------------------------\n",
            "Example 3:\n",
            "Question: What did Paul Wall offer to all U.S. Olympic Medalists?\n",
            "\n",
            "Target answer: Paul Wall wants to give free gold grills to all U.S. Olympic Medalists.\n",
            "\n",
            "Generated answer: Paul Wall offered to give free gold grills to all U.S. Olympic Medalists.\n",
            "----------------------------------------\n",
            "Example 4:\n",
            "Question: What are the main agricultural products that African countries export to the rest of the world?\n",
            "\n",
            "Target answer: African countries mainly export cocoa, edible fruit and nuts, coffee and tea, and vegetables to the rest of the world.\n",
            "\n",
            "Generated answer: African countries primarily export cocoa, edible fruit and nuts, coffee, and tea, as well as vegetables to the rest of the world. However, the continent spends between $30 billion and $50 billion annually on importing agricultural products instead of developing its productive capacities for trade. This is attributed to low yields and relatively low levels of productivity among smallholder farmers who produce 70% of Africa's food supply. To address this challenge, African governments and leaders should prioritize innovation in production technology, reduce the cost of inputs such as energy and fertilizers, and invest in agriculture-related infrastructure like credit facilities, transportation networks, cold-storage facilities, and communication networks. Additionally, access to information and knowledge transfer are crucial for improving agricultural practices in Africa. By implementing these strategies, Africa can reduce its reliance on imported agricultural products and become a major supplier of agricultural products to the rest of the world.\n",
            "----------------------------------------\n",
            "Example 5:\n",
            "Question: What is the main goal of the CHI 2011 workshop on large interactive displays in public urban contexts?\n",
            "\n",
            "Target answer: The main goal of this one-day CHI 2011 workshop is to cross-fertilize insights from different disciplines, to establish a more general understanding of large interactive displays in public urban contexts, and to develop an agenda for future research directions in this area.\n",
            "\n",
            "Generated answer: I am interested in participating in the CHI 2011 workshop on large interactive displays in public urban contexts. My research focuses on the design and evaluation of interactive exhibits for museums and public spaces, and I believe that this workshop will provide a valuable opportunity to share my findings and learn from others in the field.\n",
            "\n",
            "In my position paper, I would like to discuss my experiences with designing interactive exhibits that promote engaging experiences and go beyond playful interaction. Specifically, I would like to explore the following topics:\n",
            "\n",
            "* How to design interactive exhibits that encourage active exploration and discussion of the presented content, rather than simply inviting playful interaction.\n",
            "* How different interaction models can shape people's experience in urban spaces, and how to evaluate their impact.\n",
            "* How to evaluate the success of large interactive displays in urban spaces, and what methodological approaches are most effective in different stages of the design process.\n",
            "\n",
            "I believe that my background in human-computer interaction and exhibit design will provide a unique perspective on these topics, and I am excited to contribute to the workshop and learn from others. Thank you for the opportunity to participate!\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "p4IZruUdODy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the fine tuned model."
      ],
      "metadata": {
        "id": "aIaQgtWqxpLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_answers_fine_tuning(model, tokenizer, test_data):\n",
        "    exact_matches = []\n",
        "    f1_scores = []\n",
        "    bleu_scores=[]\n",
        "    cosine_similarity_scores=[]\n",
        "\n",
        "    for idx, row in test_data.iterrows():\n",
        "        question = row['question']\n",
        "        context = row['context']\n",
        "        target_answer = row['answer']\n",
        "\n",
        "        prompt = llama_prompt.format(question, context, \"\")\n",
        "\n",
        "        result = chatbot_fine_tuned(prompt, tokenizer, fine_tuned_model, generation_args)\n",
        "        pred_answer = extract_answer_only(result)\n",
        "\n",
        "        em = exact_match_score(pred_answer, target_answer)\n",
        "        f1 = f1_score(pred_answer, target_answer)\n",
        "        bleu = bleu_score(pred_answer, target_answer)\n",
        "        cosine_similarity_score = compute_similarity_score(pred_answer, target_answer)\n",
        "\n",
        "        exact_matches.append(em)\n",
        "        f1_scores.append(f1)\n",
        "        bleu_scores.append(bleu)\n",
        "        cosine_similarity_scores.append(cosine_similarity_score)\n",
        "\n",
        "        print(\"em: \", em)\n",
        "        print(\"f1: \", f1)\n",
        "        print(\"bleu: \", bleu)\n",
        "        print(\"cosine_similarity_scores: \", cosine_similarity_score)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return (\n",
        "      sum(bleu_scores) / len(bleu_scores) * 100,\n",
        "      sum(exact_matches) / len(exact_matches) * 100,\n",
        "      sum(f1_scores) / len(f1_scores) * 100,\n",
        "      sum(cosine_similarity_scores) / len(cosine_similarity_scores) * 100\n",
        "    )"
      ],
      "metadata": {
        "id": "LM8eG3a-Z2VH"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_bleu_fine_tuning, avg_em_fine_tuning, avg_f1_fine_tuning, cosine_similarity_score_fine_tuning= eval_answers_fine_tuning(fine_tuned_model, tokenizer, small_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iksZbn-HZ2VI",
        "outputId": "5bb2e39f-fbb3-424d-97d5-367199ca1861"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "em:  0\n",
            "f1:  1.0\n",
            "bleu:  0.7016879391277372\n",
            "cosine_similarity_scores:  0.9814428091049194\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.8163265306122449\n",
            "bleu:  0.4387328902288626\n",
            "cosine_similarity_scores:  0.9350205659866333\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.9230769230769231\n",
            "bleu:  0.8091067115702212\n",
            "cosine_similarity_scores:  0.9610199332237244\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.21383647798742136\n",
            "bleu:  0.09502552658385692\n",
            "cosine_similarity_scores:  0.7385621070861816\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.19819819819819817\n",
            "bleu:  0.0461812865601642\n",
            "cosine_similarity_scores:  0.7915804386138916\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3228346456692913\n",
            "bleu:  0.1068486020366336\n",
            "cosine_similarity_scores:  0.9212591648101807\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.13930348258706468\n",
            "bleu:  0.018088381880463782\n",
            "cosine_similarity_scores:  0.4658774137496948\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.8648648648648649\n",
            "bleu:  0.5784954323743581\n",
            "cosine_similarity_scores:  0.9309682250022888\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.22368421052631582\n",
            "bleu:  0.03801620763409172\n",
            "cosine_similarity_scores:  0.48120224475860596\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.31775700934579443\n",
            "bleu:  0.05725515867860291\n",
            "cosine_similarity_scores:  0.8281955718994141\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.8571428571428571\n",
            "bleu:  0.5325316764898671\n",
            "cosine_similarity_scores:  0.8352382183074951\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.6451612903225806\n",
            "bleu:  0.23203058032469892\n",
            "cosine_similarity_scores:  0.9486209154129028\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.509090909090909\n",
            "bleu:  0.2638794065949425\n",
            "cosine_similarity_scores:  0.8897643089294434\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.9600000000000001\n",
            "bleu:  0.8843946454355334\n",
            "cosine_similarity_scores:  0.9863824844360352\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.14883720930232558\n",
            "bleu:  0.040324521168918916\n",
            "cosine_similarity_scores:  0.6846998929977417\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.45283018867924524\n",
            "bleu:  0.197603544613522\n",
            "cosine_similarity_scores:  0.905478835105896\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.20512820512820512\n",
            "bleu:  0.05980398481493448\n",
            "cosine_similarity_scores:  0.42158639430999756\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3703703703703703\n",
            "bleu:  0.046101520617043576\n",
            "cosine_similarity_scores:  0.7262650728225708\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.25396825396825395\n",
            "bleu:  0.09021351908689755\n",
            "cosine_similarity_scores:  0.8728842735290527\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.40796019900497515\n",
            "bleu:  0.13164567009955472\n",
            "cosine_similarity_scores:  0.9244564771652222\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.05194805194805196\n",
            "bleu:  0.011303105368966513\n",
            "cosine_similarity_scores:  0.7497543692588806\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.59375\n",
            "bleu:  0.42572326734777655\n",
            "cosine_similarity_scores:  0.9493030309677124\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.20512820512820512\n",
            "bleu:  0.055643894473000985\n",
            "cosine_similarity_scores:  0.8419011831283569\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.3257918552036199\n",
            "bleu:  0.09246209946748563\n",
            "cosine_similarity_scores:  0.8255676627159119\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.17283950617283952\n",
            "bleu:  0.058181486255693166\n",
            "cosine_similarity_scores:  0.7576203346252441\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.30952380952380953\n",
            "bleu:  0.10011368173026872\n",
            "cosine_similarity_scores:  0.8396016359329224\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.5194805194805195\n",
            "bleu:  0.19247512930869096\n",
            "cosine_similarity_scores:  0.8887204527854919\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.18181818181818182\n",
            "bleu:  0.03761108257961349\n",
            "cosine_similarity_scores:  0.9133042097091675\n",
            "----------------------------------------\n",
            "em:  0\n",
            "f1:  0.49756097560975604\n",
            "bleu:  0.273134425081755\n",
            "cosine_similarity_scores:  0.903637170791626\n",
            "----------------------------------------\n",
            "em:  1\n",
            "f1:  1.0\n",
            "bleu:  1.0\n",
            "cosine_similarity_scores:  1.0000001192092896\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average BLEU score: {avg_bleu_fine_tuning:.2f}\")\n",
        "print(f\"Exact Match: {avg_em_fine_tuning:.2f}%\")\n",
        "print(f\"F1 Score: {avg_f1_fine_tuning:.2f}%\")\n",
        "print(f\"Cosine Similarity Score: {cosine_similarity_score_fine_tuning:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq43GGrEZ2VI",
        "outputId": "664cb289-6908-4833-a14b-636ff0d95f1b"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 25.38\n",
            "Exact Match: 3.33%\n",
            "F1 Score: 45.63%\n",
            "Cosine Similarity Score: 83.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison among all the models"
      ],
      "metadata": {
        "id": "OHWyqEug7e5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this final section we compare all the models."
      ],
      "metadata": {
        "id": "DD2mjJ8-7mqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with the metric values\n",
        "data = {\n",
        "    \"EM\": [avg_em_zero_shot, avg_em_one_shot, avg_em_fine_tuning],\n",
        "    \"F1\": [avg_f1_zero_shot, avg_f1_one_shot, avg_f1_fine_tuning],\n",
        "    \"BLEU\": [avg_bleu_zero_shot, avg_bleu_one_shot, avg_bleu_fine_tuning],\n",
        "    \"Cosine Similarity\": [\n",
        "        cosine_similarity_score_zero_shot,\n",
        "        cosine_similarity_score_one_shot,\n",
        "        cosine_similarity_score_fine_tuning\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Labels for the rows\n",
        "index = [\"Zero-shot\", \"One-shot\", \"Fine-tuning\"]\n",
        "\n",
        "# Create and display the table\n",
        "df = pd.DataFrame(data, index=index)\n",
        "df = df.round(2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3es5JOc2dHzT",
        "outputId": "a3976e7e-ad48-403e-ed8f-2c804fb23026"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               EM     F1   BLEU  Cosine Similarity\n",
            "Zero-shot    6.67  36.94  18.89              79.79\n",
            "One-shot     6.67  49.85  28.49              86.13\n",
            "Fine-tuning  3.33  45.63  25.38              83.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show an interesting pattern across the zero-shot, one-shot, and fine-tuned settings.\n",
        "\n",
        "* **Exact Match (EM)**: All approaches performed poorly on exact match, with scores around 3–7%. This suggests that generating answers that exactly match the reference text is particularly challenging, likely due to the high variability in natural language expression. Interestingly, fine-tuning slightly underperforms zero- and one-shot in EM, possibly because the model learned to generalize more than to copy exact phrasings.\n",
        "\n",
        "* **F1 Score**: One-shot learning achieves the highest F1 score (49.85%), significantly outperforming zero-shot (36.94%) and fine-tuning (45.63%). This may indicate that a well-crafted, in-context example helps the model better align with the format and structure of the expected answer, capturing more relevant tokens even if the phrasing varies.\n",
        "\n",
        "* **BLEU Score**: BLEU scores follow a similar trend: one-shot > fine-tuning > zero-shot. This supports the idea that a single relevant example helps guide the model toward more syntactically and lexically similar responses.\n",
        "\n",
        "* **Cosine Similarity**: All three methods show relatively high cosine similarity (above 79%), indicating that even when the outputs are not lexically identical, they remain semantically similar. One-shot achieves the highest semantic similarity, which is consistent with its strong F1 and BLEU scores.\n",
        "\n",
        "**Conclusion**:\n",
        "While fine-tuning provides better fluency and task-specific learning, in this case, one-shot prompting surprisingly outperforms it in F1, BLEU, and semantic similarity. This could be due to a small fine-tuning dataset, suboptimal training, or the fact that the model already had strong general capabilities that were better leveraged through prompting than additional supervised training."
      ],
      "metadata": {
        "id": "gIN7Htipo18S"
      }
    }
  ]
}