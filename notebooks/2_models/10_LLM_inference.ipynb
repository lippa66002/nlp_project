{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "8nORPeY4I96P",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "55b46449-7ff3-4a8e-cf84-56590139a0ba"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.chdir(f'/content/drive/MyDrive/Polimi/NLP')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0Ab2-9FKJSBa",
    "outputId": "9aada754-3452-4e17-beca-8dc2bf3f53cc"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/Polimi/NLP'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data upload and import libreries"
   ],
   "metadata": {
    "id": "EIwxoNLEjkM2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q \"transformers\" \"peft\" \"accelerate\" \"bitsandbytes\" \"trl\" \"safetensors\" \"tiktoken\"\n",
    "!pip install -q -U langchain\n",
    "!pip install -q langchain-community langchain-core\n",
    "!pip install -q --no-deps peft\n",
    "!pip install -q lightning"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T15:40:42.308015Z",
     "iopub.execute_input": "2025-05-21T15:40:42.308255Z",
     "iopub.status.idle": "2025-05-21T15:42:01.003756Z",
     "shell.execute_reply.started": "2025-05-21T15:40:42.308232Z",
     "shell.execute_reply": "2025-05-21T15:42:01.003075Z"
    },
    "id": "aHpbbrYPjkM2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ebcc85d8-80d7-4228-c931-9f7aa12c683d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from peft import LoraConfig\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T15:42:14.219133Z",
     "iopub.execute_input": "2025-05-21T15:42:14.219934Z",
     "iopub.status.idle": "2025-05-21T15:42:40.143610Z",
     "shell.execute_reply.started": "2025-05-21T15:42:14.219889Z",
     "shell.execute_reply": "2025-05-21T15:42:40.142846Z"
    },
    "id": "9VYs9WwYjkM3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b5f33b1a-a504-4941-f3bb-81fdc903db00"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Arnv53hIGISu",
    "outputId": "acae02c0-de2e-426a-bb6e-29479280afe5"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load of the data"
   ],
   "metadata": {
    "id": "xb5FhLnxptNV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_set = pd.read_parquet(\"tuning_data.parquet\")\n",
    "test_set = pd.read_parquet(\"test_data.parquet\")\n",
    "small_test_set = test_set[:15]"
   ],
   "metadata": {
    "id": "t39cd7KakixP"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load of the model"
   ],
   "metadata": {
    "id": "1YCBJyW3jkM4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we load the model and his tokenizer."
   ],
   "metadata": {
    "id": "XgSD7dL6jkM4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\""
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T15:42:40.618877Z",
     "iopub.execute_input": "2025-05-21T15:42:40.619240Z",
     "iopub.status.idle": "2025-05-21T15:42:40.622507Z",
     "shell.execute_reply.started": "2025-05-21T15:42:40.619213Z",
     "shell.execute_reply": "2025-05-21T15:42:40.621906Z"
    },
    "id": "Ln3d5CbIjkM4"
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is too big, so we will make use of a 4-bit version of the model."
   ],
   "metadata": {
    "id": "-5BypDJjjkM4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BitsAndBytesConfig int-4 config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T15:42:40.623205Z",
     "iopub.execute_input": "2025-05-21T15:42:40.623415Z",
     "iopub.status.idle": "2025-05-21T15:42:40.635942Z",
     "shell.execute_reply.started": "2025-05-21T15:42:40.623401Z",
     "shell.execute_reply": "2025-05-21T15:42:40.635467Z"
    },
    "id": "u-P6LNLLjkM4"
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the model"
   ],
   "metadata": {
    "id": "9KJ4UMc7zove"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "681abfac53d54f0ea70c6309b5342a29",
      "838122ee2f154708973ef5a3619520e3",
      "116d372f81ba49a387b2c8fcd420a01d",
      "3ffb2b78fe334e9281879ced3f429a2f",
      "4ee9967cd8334f51b28d8df44b6ecdcc",
      "d668106f617e4f7c93441adb80257f5a",
      "153c5d62259747d9897a5e201273f7c1",
      "e28f43ac85874afdbfb7bcddee769934",
      "ad149d49fa644602979daf97aa394cda",
      "b71c014790564091b1d11229091a50be",
      "816324bf17e34b04ab2559848f5b7f5f",
      "05a264665e0d40ee9f27441cf716be28",
      "732e0395f2ca43ba855a8c7a8e656b3c",
      "57bbe06586954de9ac0208d85de1c0f1",
      "1dbac997b39c4a0b9563ccfa5965df9d",
      "d45101055240422ab3c911a98b6f56b7",
      "70521928e50f473b8dfbd57967afb4b4",
      "b70eb405e8a042199e5ec0b701eb4bd4",
      "f62e080354e9410c95be525076d0fb3b",
      "7fd191614a29420980c0040e7260c6fa",
      "f6ad9232a6174bc7847de8347b998fc9",
      "5da3f4e031fe4433b0fcd8fbf012afe3",
      "f697432b9cbf40f0b7c0b4213638d5a1",
      "3c2b5205643f43559cf09a0020ec2613",
      "9d8fc396c8a64f2796c0af696b42c82a",
      "059ca64491b745d395174ad5b9bd37be",
      "c9a9a3f72fbb44fb979a368ad2fe0af8",
      "01965136ae73465390b17ad74d12ad9e",
      "7b714673d90d47f0b03ac79d3ce15ae3",
      "ea27e6f36bc44a13b1c4dbd24da38964",
      "e62fefe906d642c9baffcd5fef7fcea2",
      "6cba2575a49649e4b225ad8c3d6c5736",
      "70dfa2f7fa5f49e9aa2d1158adf19b19",
      "8a8490459cb24aeb8e8b2717dd2ca4d5",
      "e7afd0681f9646c3b4e7a27db75e2f03",
      "acd54360aa9b4bf9904e0eeaf04abdc7",
      "f4cb19db4680465b8463032ec1b66401",
      "2321a5ef1b7045cab4e38f59707cecb4",
      "4da9719543a148d1953bf12ba1a3818b",
      "e9eadf26a15c4597ba0e17de475c13ac",
      "222b357a75f6430fafc8dbb23c7d47f7",
      "ad73a4789ef04121b547b52d29d53870",
      "3bf87a9770234d7083fe971e8ebbf8a7",
      "e3a2b0b90cb346c79e45a988aae3f234",
      "184b900d88f043c6af8a10861be8b150",
      "f0a072bfcd864e2290688619a6243611",
      "f441f045cfd349c789f4936c8280d8d1",
      "a20a4a8f94b349769478744a5142babb",
      "29d5f019daea4cc19c2bb0f17b3e3225",
      "a93d34b7577a4816989367e909cd4fd8",
      "ea7fee9f50d04149ba3f22f94778f49d",
      "2c49924f9ae6437085d00620855625d1",
      "804b924e4be54c14ae9f537a1c9c4a8c",
      "2ca6e4e80a5943bdb908a68139c56fc6",
      "e539f0c40e63495f983d1613eb591327",
      "5f1dfece599a45548cccb10c51a7f6e4",
      "7a40e72bb2334bc595c6384db85594db",
      "c5b3d96776fb4abe9c49dfd38429ded6",
      "23a9bf6ebaad4e7ca49357d0a23e5ef6",
      "d0f39b200e9f4e13ba48f3d81fbb9977",
      "6254eaba1ec441848501ef3cd1a50436",
      "03b5c759c5594158883c73a1ab3d0e70",
      "a2ba588a3729479496e8f6d39b1ac736",
      "5cf016b4d8c341b0bc665e7aa286c979",
      "decc918bd5834d7db0f1ecb6a6663533",
      "c8653b23368447079d96819c465eda31",
      "b277bbbb46314d1685bfd14ba2678b34",
      "98ceceeea8e446b8afbcfe26d864c6c6",
      "b6dd0ba8deda455483ddbda4927af9db",
      "6ce8f5be668e49b4bb284dd43fd6be78",
      "d65ffff726e54809ac7abb12671318cd",
      "c6ff9a18071a40dda673f0ed18fbff1c",
      "f825ef404b1840bfabfc1066d1eef74a",
      "4a05951733644e58920c0e33fc9e16f1",
      "84dbfdf749b04f65a8913b2fb49aa785",
      "db9076a9ef3d49ad8c8b465716e353e2",
      "ebfadc2f7ac7458ab5b0647e4afd9581"
     ]
    },
    "id": "rVR5mxZfEJfx",
    "outputId": "fcfa012e-50d1-480f-a42b-e5ba417f0cc5"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "681abfac53d54f0ea70c6309b5342a29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05a264665e0d40ee9f27441cf716be28"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f697432b9cbf40f0b7c0b4213638d5a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a8490459cb24aeb8e8b2717dd2ca4d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "184b900d88f043c6af8a10861be8b150"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f1dfece599a45548cccb10c51a7f6e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b277bbbb46314d1685bfd14ba2678b34"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the weights"
   ],
   "metadata": {
    "id": "jrgcW0lq0Fm7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "state_dict = torch.load(\"model2_Llama.pth\", map_location=\"cuda\")\n",
    "fine_tuned_model.load_state_dict(state_dict, strict=False)\n",
    "fine_tuned_model.to(\"cuda\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zzmx-XmlzJHp",
    "outputId": "2b6f78f5-ebe9-4e5d-a14d-8b9fa772fe1e"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the tokenizer"
   ],
   "metadata": {
    "id": "9u21YbhN0JGY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-21T15:43:43.135091Z",
     "iopub.execute_input": "2025-05-21T15:43:43.135415Z",
     "iopub.status.idle": "2025-05-21T15:43:47.016615Z",
     "shell.execute_reply.started": "2025-05-21T15:43:43.135391Z",
     "shell.execute_reply": "2025-05-21T15:43:47.015765Z"
    },
    "id": "pozTU87xjkM5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "07cc0e8d90bd4ca6b7f4806bc473251e",
      "6737a9fca9b84da895cd9450f582aa6d",
      "cb8f8f94005b40f0a559a005d1295517",
      "badbae35fb5d472a976b9b213ffba780",
      "e699f40fef384083840c4a9067561119",
      "da620fdcc1e64cc882b5535d1e2605bf",
      "234a4fffd0df4af999d51ac0dc7c5b16",
      "b3ad1965efce4d67b128395cf06feb8f",
      "5494b19a1bbc4aaebc24a933a7a91c90",
      "8c2fa0acd8344548a1315004bab18561",
      "61574e28f40f46fb8792a8ba6d6c2265",
      "e02d540f85c142f09affa09f01160c62",
      "dc2d620430754fe1aebef84c3082a5ce",
      "2f78034042814442b9f511fecdbdb3f3",
      "74626851d0bd4929a8819e34e9151925",
      "caafa66ffe7042be82921ae4acd542c2",
      "1c6ab6b0429744318e67c038da589898",
      "9864928ab7bb42f18936027d10ffa369",
      "667bd72f602d44b1be95cd529a31225d",
      "0c7c96cc484d44d2a764f82fc6d7db32",
      "93d479bfd3bf460787cef982016bda85",
      "f590a9ad5df741478e6834d99201bb31",
      "dfafcb076bae4164bf87ab57b7bd9c69",
      "985c27ce982f4013aeff43d847b0d2f4",
      "7b3186f49f2b495a87b8e460e66579d4",
      "9c4db209ced24267bdda912afb9f765f",
      "203fbdeeb0fe40efa702441e149630e9",
      "623ed28323ce4301b7ffbafc32cbea7f",
      "ba5f8df6a1b44f84bb357af5203cfddb",
      "e358268b87754f5898eaa2e77bdf26e6",
      "e7885fb34bbc450b99e07c2f7cf300d4",
      "a894849287dd42ccaea82435fdb83dfb",
      "5f8d8ba8283f4f9fa030cf51c5dc4451",
      "ed85815b4f864569b89629bddb970fe1",
      "419646f4c33f4e9abcc8ed7afa03fa05",
      "4fbe935674a04e9390aa97d66bf78b69",
      "6b3be61a9f154d6b85a45365e3ad432c",
      "da0b8f7082134bc3b45813c260ca2345",
      "2d33ccc793934a1f81a6614ddd565426",
      "cd5c8fbf8c8b4037b0c524817a540f97",
      "d0deaea3c6cd48738d3492271753a478",
      "a4b7d93888934937b6566145a14e7c9b",
      "bde27413930c487e8d6e4715a826fc68",
      "b8ca8989e604487b80524eb87aa2f913",
      "29171d53b161485f898ad65a1a7a6e45",
      "a31da41eb02541cabd7d1a8456955145",
      "282330b779384a7cb7f5e67ce3a41686",
      "1c9dbad8d9784efeb2849a1371a29bce",
      "1b7691b491674dfdb70bc3fae39f50dd",
      "477a6afad4f844ec81f1d043c6e0aa1c",
      "629b473581cc463a929b572c7eee5468",
      "10201bd24a9b4f8b8fb7f9520944ab0b",
      "18a90754a78049f4a8cd3f21389bb59b",
      "c8aaf6bb58aa4dfcbd9960a957f88e28",
      "a2e55438b1ac412a9a8124d58546fdbd"
     ]
    },
    "outputId": "96ce8059-14d0-4183-e650-13fdf9ca986c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07cc0e8d90bd4ca6b7f4806bc473251e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e02d540f85c142f09affa09f01160c62"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfafcb076bae4164bf87ab57b7bd9c69"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed85815b4f864569b89629bddb970fe1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29171d53b161485f898ad65a1a7a6e45"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt format for LLaMA-2 chat models\n",
    "llama_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ],
   "metadata": {
    "id": "LbCmDnwTlWLf"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "id": "PTRvXjdpLdaF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the fine tuned model and see if there are any improvements."
   ],
   "metadata": {
    "id": "Bq09w0f-M5z2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first load our model with the weights."
   ],
   "metadata": {
    "id": "H8Tpr85tNBFs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generation_args_fine_tuned = {\n",
    "    \"max_new_tokens\": 3000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"do_sample\": True,\n",
    "}"
   ],
   "metadata": {
    "id": "QXwAcKYrb8Kk"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the function that generates and returns the answer.The genereted answer may include not only the direct answer but also additional context,\n",
    "explanations, or formatting."
   ],
   "metadata": {
    "id": "n_RgpmdA0S0d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def chatbot_fine_tuned(prompt, tokenizer, model, generation_args):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, **generation_args_fine_tuned)\n",
    "\n",
    "    # Decode generated tokens\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "id": "WeTl-j-aMrL5"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a function that extracts only the answer to the question from the generated response."
   ],
   "metadata": {
    "id": "A1LasMQL0WnR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_answer_only(generated_text):\n",
    "    # Cerca il delimitatore \"### Response:\" e restituisce solo ciò che viene dopo\n",
    "    if \"### Response:\" in generated_text:\n",
    "        return generated_text.split(\"### Response:\")[1].strip()\n",
    "    elif \"[/INST]\" in generated_text:\n",
    "        return generated_text.split(\"[/INST]\")[-1].strip()\n",
    "    else:\n",
    "        return generated_text.strip()"
   ],
   "metadata": {
    "id": "YDI7Xjl_2N6g"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test on few samples."
   ],
   "metadata": {
    "id": "xBdsHUes0ZbJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n = 1\n",
    "\n",
    "for i, row in test_set.head(n).reset_index(drop=True).iterrows():\n",
    "    question = row['question']\n",
    "    context = row['context']\n",
    "    target_answer = row['answer']\n",
    "\n",
    "    prompt = llama_prompt.format(question, context, \"\")\n",
    "\n",
    "    # Generate the answer\n",
    "    result = chatbot_fine_tuned(prompt, tokenizer, fine_tuned_model, generation_args_fine_tuned)\n",
    "    clean_answer = extract_answer_only(result)\n",
    "\n",
    "    # Output\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(\"Question:\", question)\n",
    "    print(\"\\nTarget answer:\", target_answer)\n",
    "    print(\"\\nGenerated answer:\", clean_answer)\n",
    "    print(\"-\" * 40)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oth1dmXgLgVj",
    "outputId": "8e238a13-ee86-4925-8edc-fccbc35ceeef"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Example 1:\n",
      "Question: Who is the music director of the Quebec Symphony Orchestra?\n",
      "\n",
      "Target answer: The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n",
      "\n",
      "Generated answer: Fabien Gabel is the music director of the Quebec Symphony Orchestra.\n",
      "----------------------------------------\n"
     ]
    }
   ]
  }
 ]
}