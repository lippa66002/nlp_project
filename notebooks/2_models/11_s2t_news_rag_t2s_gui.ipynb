{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#0. Introduction"
      ],
      "metadata": {
        "id": "B6PP_FqRSmU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to build a conversational interface that lets users interact with a Retrieval-Augmented Generator (RAG) agent by **voice** or **text**, retrieve up-to-date news articles, and hear the responses via text-to-speech.  \n",
        "\n",
        "**Key features:**  \n",
        "- **Speech-to-Text (STT):** Record audio and transcribe user speech into text  \n",
        "- **Retrieval-Augmented Generation:** Use LangChain’s agent to fetch relevant news snippets and compose answers  \n",
        "- **Text-to-Speech (TTS):** Convert the agent’s reply into spoken audio  \n",
        "- **Dual Input Modes:** Switch between typing your query or speaking it, with a unified chat history  \n",
        "- **Custom UI:** Gradio Blocks layout with theming, CSS customizations, and a toggled voice player  "
      ],
      "metadata": {
        "id": "232kzgRKSkl0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsIScC6fg4r7"
      },
      "source": [
        "#1. Importations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f07500e"
      },
      "source": [
        "Install required Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdHqkTYdF-yV",
        "outputId": "ba026daa-e4f3-4b95-be7b-ea3bed8361b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m454.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-google-genai feedparser google-generativeai --quiet\n",
        "!pip install openai-whisper ffmpeg-python --quiet\n",
        "!pip install torch torchaudio --quiet\n",
        "!pip install numpy scipy librosa unidecode inflect --quiet\n",
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d23e9ec"
      },
      "source": [
        "General Python and PyTorch imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9riV0f0GJVRh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy\n",
        "import os\n",
        "import io\n",
        "import ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3cabfd6"
      },
      "source": [
        "Import LangChain core classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bHkx922kJSjC"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
        "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
        "from langchain.agents import AgentExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b83a04a"
      },
      "source": [
        "Load Gemini API key and set environment variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XOlwlmLHIDWV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f88bc52"
      },
      "source": [
        "Select GPU device if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A4qUp5sNItgO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s3Xg4HVI3wd"
      },
      "source": [
        "#2. Speech To Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61a71385"
      },
      "source": [
        "Import Whisper for speech‑to‑text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ErD7m-Y2I7Hk"
      },
      "outputs": [],
      "source": [
        "import whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897c7e0d"
      },
      "source": [
        "Load Whisper STT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yd8M-p3I-eZ",
        "outputId": "895ccc1d-0ab3-4fb4-df55-e37cd480e283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 124MiB/s]\n"
          ]
        }
      ],
      "source": [
        "MODEL_SIZE = \"base\"\n",
        "stt_model = whisper.load_model(MODEL_SIZE, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd532d1c"
      },
      "source": [
        "Helper function to transcribe an audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GbfKu9r6JEGC"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio_filepath(audio_filepath):\n",
        "    result = stt_model.transcribe(audio_filepath)\n",
        "    transcribed_text = result[\"text\"].strip()\n",
        "    return transcribed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ubmOaTmJIQK"
      },
      "source": [
        "#3. RAG news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52eb880c"
      },
      "source": [
        "Import feedparser for RSS parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XyOufUQWJKmt"
      },
      "outputs": [],
      "source": [
        "import feedparser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e69eb6e6"
      },
      "source": [
        "Define Google News search tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a4JsuDE3JMsj"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def search_google_news(topic: str, max_results: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Searches Google News RSS feed for a given topic and returns recent headlines.\n",
        "    Args:\n",
        "        topic (str): The topic to search for (e.g., \"artificial intelligence\", \"climate change\").\n",
        "        max_results (int): The maximum number of news headlines to return.\n",
        "    Returns:\n",
        "        str: A formatted string containing the news headlines and their links,\n",
        "             or a message if no news is found or an error occurs.\n",
        "    \"\"\"\n",
        "    if not topic:\n",
        "        return \"Error: Please provide a topic to search for.\"\n",
        "    try:\n",
        "        safe_topic = topic.replace(\" \", \"+\")\n",
        "        url = f\"https://news.google.com/rss/search?q={safe_topic}&hl=en-US&gl=US&ceid=US:en\"\n",
        "        feed = feedparser.parse(url)\n",
        "\n",
        "        if not feed.entries:\n",
        "            return f\"No recent news found for '{topic}'.\"\n",
        "\n",
        "        headlines = []\n",
        "        for i, entry in enumerate(feed.entries):\n",
        "            if i >= max_results:\n",
        "                break\n",
        "            title = entry.title\n",
        "            link = entry.link\n",
        "\n",
        "            headlines.append(f\"  - Title: {title}\\n    Link: {link}\")\n",
        "\n",
        "        return f\"Recent news for '{topic}':\\n\" + \"\\n\".join(headlines)\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching news for '{topic}': {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ea96e93"
      },
      "source": [
        "Configure Gemini model with the news tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JqnaSFb-w-qi"
      },
      "outputs": [],
      "source": [
        "rag_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.2, google_api_key=GOOGLE_API_KEY)\n",
        "rag_tools = [search_google_news]\n",
        "rag_llm_with_tools = rag_llm.bind_tools(rag_tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2c7ec0"
      },
      "source": [
        "Create the prompt template for the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jyBibQmWJdvb"
      },
      "outputs": [],
      "source": [
        "rag_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. You have access to a tool called 'search_google_news' \"\n",
        "            \"which can find recent news headlines on a given topic. \"\n",
        "            \"When a user asks for news, you should use this tool. \"\n",
        "            \"After getting the news headlines from the tool, present them to the user in a readable format. \"\n",
        "            \"If the user asks something not related to news, try to answer directly.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eed1a0b5"
      },
      "source": [
        "Assemble the LangChain agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "APLbpKTZJggx"
      },
      "outputs": [],
      "source": [
        "rag_agent = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(x[\"intermediate_steps\"]),\n",
        "        \"chat_history\": lambda x: x.get(\"chat_history\", []),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | rag_llm_with_tools\n",
        "    | OpenAIToolsAgentOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90d9c98c"
      },
      "source": [
        "Wrap the agent in an executor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SlYzbyldJinZ"
      },
      "outputs": [],
      "source": [
        "rag_agent_executor = AgentExecutor(\n",
        "    agent=rag_agent,\n",
        "    tools=rag_tools,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b20f139"
      },
      "source": [
        "Print confirmation of agent initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f99xf0sJkgy",
        "outputId": "46d0f7f0-151a-4587-85f6-292df52b0cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG News Agent initialized.\n"
          ]
        }
      ],
      "source": [
        "print(\"RAG News Agent initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKUWNc0dJmCK"
      },
      "source": [
        "#4. Text To Speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23a90caf"
      },
      "source": [
        "Load Tacotron2 TTS model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w7LFd83xJv56",
        "outputId": "1d90fbf0-5293-4b3e-ae04-0e0a6ee4cfd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.09.0/files/nvidia_tacotron2pyt_fp16_20190427\n"
          ]
        }
      ],
      "source": [
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
        "tacotron2 = tacotron2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ebee07"
      },
      "source": [
        "Load WaveGlow vocoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uEks_BzpJ1i2",
        "outputId": "750e7e6d-f8f7-4631-8614-0ab5ac77e454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp/versions/19.09.0/files/nvidia_waveglowpyt_fp16_20190427\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        }
      ],
      "source": [
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ce4dc1"
      },
      "source": [
        "Load additional TTS utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z40tHyQ4J6Om",
        "outputId": "45aec71e-68ce-4293-970c-e92a4633b372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
          ]
        }
      ],
      "source": [
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "922b5605"
      },
      "source": [
        "Filename constants for TTS output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AwN9lmfcJ--w"
      },
      "outputs": [],
      "source": [
        "from scipy.io.wavfile import write as write_wav\n",
        "TTS_OUTPUT_FILENAME = \"temp_tts_output.wav\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e1d1b2"
      },
      "source": [
        "Helper to synthesize speech to a WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IhsJaysXKA_6"
      },
      "outputs": [],
      "source": [
        "def synthesize_speech_to_file(text_to_speak):\n",
        "    # create a silent audio file if no text has been transcribed\n",
        "    if not text_to_speak or text_to_speak.strip() == \"\":\n",
        "        print(\"TTS: Empty text, skipping synthesis.\")\n",
        "        sample_rate = 22050\n",
        "        silent_audio = np.zeros(int(0.1 * sample_rate)) # 0.1 second of silence\n",
        "        write_wav(TTS_OUTPUT_FILENAME, sample_rate, silent_audio.astype(np.int16))\n",
        "        return TTS_OUTPUT_FILENAME\n",
        "\n",
        "    sequences, lengths = utils.prepare_input_sequence([text_to_speak])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "        audio_tensor = waveglow.infer(mel)\n",
        "\n",
        "    audio_numpy = audio_tensor[0].data.cpu().numpy()\n",
        "    rate = 22050\n",
        "\n",
        "    write_wav(TTS_OUTPUT_FILENAME, rate, audio_numpy)\n",
        "    print(f\"Speech synthesized and saved to {TTS_OUTPUT_FILENAME}\")\n",
        "    return TTS_OUTPUT_FILENAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGvR43LNuCF4"
      },
      "source": [
        "# 5. GUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b53cb7a4"
      },
      "source": [
        "Install Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6JPL_bAfuDuc"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4ff1bf"
      },
      "source": [
        "Import Gradio and wire helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HT9IcAhKu_1y"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def stt(audio_path):\n",
        "    return stt_model.transcribe(audio_path)[\"text\"]\n",
        "\n",
        "def tts(text):\n",
        "    return synthesize_speech_to_file(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae233d6c"
      },
      "source": [
        "Define helper that calls the agent with chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "siSNjtnovE2C"
      },
      "outputs": [],
      "source": [
        "def llm(text, history_pairs):\n",
        "    \"\"\"\n",
        "    text           : last user utterance (str)\n",
        "    history_pairs  : [(user, bot), …] coming from gr.State\n",
        "    \"\"\"\n",
        "    # convert Gradio’s list of tuples into LangChain messages\n",
        "    chat_history = []\n",
        "    for human, bot in history_pairs:\n",
        "        chat_history.append(HumanMessage(content=human))\n",
        "        chat_history.append(AIMessage(content=bot))\n",
        "\n",
        "    resp = rag_agent_executor.invoke(\n",
        "        {\"input\": text, \"chat_history\": chat_history}\n",
        "    )\n",
        "    return resp[\"output\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed523665"
      },
      "source": [
        "Define UI theme and custom CSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "C0hKcwMICFfa"
      },
      "outputs": [],
      "source": [
        "theme = gr.themes.Soft(primary_hue=\"indigo\", neutral_hue=\"slate\")\n",
        "\n",
        "css = \"\"\"\n",
        "/* pagina */\n",
        ".gradio-container{\n",
        "    background:linear-gradient(135deg,#f9fbff 0%,#ffffff 100%);\n",
        "}\n",
        "/* titolo */\n",
        "#title-bar{\n",
        "    font-size:2rem;font-weight:700;text-align:center;margin:0.5rem 0;\n",
        "}\n",
        "/* microfono e altri pulsanti */\n",
        ".gr-button{\n",
        "    border-radius:9999px;font-weight:600;padding:0.6rem 1.4rem;\n",
        "    box-shadow:0 2px 4px rgba(0,0,0,0.08);\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97918468"
      },
      "source": [
        "Callback: voice interaction (audio → text → answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eOIl5PB4Dlqi"
      },
      "outputs": [],
      "source": [
        "  def voice_chat(audio, history):\n",
        "      user_msg = stt(audio)\n",
        "      bot_msg  = llm(user_msg, history)\n",
        "      history += [(user_msg, bot_msg)]\n",
        "      bot_voice = tts(bot_msg)\n",
        "      return history, bot_voice, history, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f58f739e"
      },
      "source": [
        "Callback: text interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tXC7zKTkEPj9"
      },
      "outputs": [],
      "source": [
        "def text_chat(user_msg, history):\n",
        "    bot_msg  = llm(user_msg, history)\n",
        "    history += [(user_msg, bot_msg)]\n",
        "    bot_voice = tts(bot_msg)\n",
        "    return \"\", history, bot_voice, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc22cf7b"
      },
      "source": [
        "Build the Gradio interface with tabs and toggles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeKYr7w9CJwj",
        "outputId": "c57d4a0c-e46b-4c02-f9b9-2f2aee09b8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-065012a72843>:4: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  out_chat = gr.Chatbot(label=\"Dialog\")\n"
          ]
        }
      ],
      "source": [
        "with gr.Blocks(theme=theme, css=css, title=\"Voice & Text Chat\") as demo:\n",
        "    gr.HTML(\"<div id='title-bar'>🌿😝⚙️🌿😝⚙️ - Voice & Text Chat by RAGaoutille - 🌿😝⚙️🌿😝⚙️ </div>\")\n",
        "\n",
        "    out_chat = gr.Chatbot(label=\"Dialog\")\n",
        "    with gr.Accordion(\"Assistant voice (click to toggle)\", open=False):\n",
        "        out_voice = gr.Audio(label=\"\", interactive=False)\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        with gr.Tab(\"Text\"):\n",
        "            inp_text = gr.Textbox(\n",
        "                placeholder=\"💬  Type a message and press Enter …\",\n",
        "                show_label=False,\n",
        "                lines=1,\n",
        "            )\n",
        "            inp_text.submit(\n",
        "                text_chat,\n",
        "                [inp_text, state],\n",
        "                [inp_text, out_chat, out_voice, state],\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Voice\"):\n",
        "            inp_audio = gr.Audio(\n",
        "                sources=\"microphone\",\n",
        "                type=\"filepath\",\n",
        "                label=\"Speak here\",\n",
        "            )\n",
        "\n",
        "    inp_audio.stop_recording(\n",
        "        voice_chat,\n",
        "        [inp_audio, state],\n",
        "        [out_chat, out_voice, state, inp_audio],   # resets recorder\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ea41df"
      },
      "source": [
        "Launch the Gradio app. Very important: run this cell once!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "O8X9_Vi6vH0b",
        "outputId": "779166f9-676e-49fb-b8ba-f9d3d103c59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1127003130aaedecd0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1127003130aaedecd0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "demo.queue().launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}